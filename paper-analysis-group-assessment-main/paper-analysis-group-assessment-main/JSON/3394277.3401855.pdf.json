{"TYPE": "document", "VALUE": [{"TYPE": "body", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A Smoothed Particle Hydrodynamics Mini-App for Exascale"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Aur\u00e9lien Cavelan "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "University of Basel "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "aurelien.cavelan@unibas.ch"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1440, "right": 1296, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Michal Grabarczyk "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "University of Basel "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "michalprzemyslaw.grabarczyk@unibas.ch"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 864, "right": 720, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ABSTRACT"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The Smoothed Particles Hydrodynamics (SPH) is a particle-based, meshfree, Lagrangian method used to simulate multidimensional fluids with arbitrary geometries, most commonly employed in as-trophysics, cosmology, and computational fluid-dynamics (CFD). It is expected that these computationally-demanding numerical simu-lations will significantly benefit from the up-and-coming Exascale computing infrastructures, that will perform 1018FLOP/s. In this work, we review the status of a novel SPH-EXA mini-app, which is the result of an interdisciplinary co-design project between the fields of astrophysics, fluid dynamics and computer science, whose goal is to enable SPH simulations to run on Exascale systems. The SPH-EXA mini-app merges the main characteristics of three state-of-the-art parent SPH codes (namely ChaNGa, SPH-flow, SPHYNX) with state-of-the-art (parallel) programming, optimization, and par-allelization methods. The proposed SPH-EXA mini-app is a C++14 lightweight and flexible header-only code with no external software dependencies. Parallelism is expressed via multiple programming models, which can be chosen at compilation time with or without accelerator support, for a hybrid process+thread+accelerator con-figuration. Strong- and weak-scaling experiments on a production supercomputer show that the SPH-EXA mini-app can be efficiently executed with up 267 million particles and up to 65 billion particles in total on 2,048 hybrid CPU-GPU nodes."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "CCS CONCEPTS"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022 Computing methodologies \u2192 Distributed algorithms; Par-allel algorithms; \u2022 Applied computing \u2192 Physics; \u2022 Software and its engineering;"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "KEYWORDS"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "SPH, mini-app, parallelization, performance, algorithms, Exascale"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Reference Format: "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Aur\u00e9lien Cavelan, Rub\u00e9n M. Cabez\u00f3n, Michal Grabarczyk, and Florina M. Ciorba. 2020. A Smoothed Particle Hydrodynamics Mini-App for Exascale. In Proceedings of the Platform for Advanced Scientific Computing Conference"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 248, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PASC '20, June 29-July 1, 2020, Geneva, Switzerland"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "\u00a9 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 288, "firstLine": 10}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM ISBN 978-1-4503-7993-9/20/06...$15.00"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 2448, "hanging": 4}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Rub\u00e9n M. Cabez\u00f3n "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "University of Basel "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "ruben.cabezon@unibas.ch"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1296, "right": 1440, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Florina M. Ciorba "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "University of Basel "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "florina.ciorba@unibas.ch"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1296, "right": 1584, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(PASC '20), June 29-July 1, 2020, Geneva, Switzerland. ACM, New York, NY, USA, 11 pages."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "hanging": 10}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "INTRODUCTION"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The Smoothed Particle Hydrodynamics is a commonly used method to simulate the mechanics of continuum media. This method is a pure Lagrangian technique, particle-based, meshfree, and adequate for simulating highly distorted geometries and very dynamical sce-narios, while conserving momentum and energy by construction. As such, it has been used in many different fields, including com-putational fluid dynamics, solid mechanics, engineering, nuclear fusion, astrophysics, and cosmology. In this work we present a review of the status of an SPH mini-app designed for Exascale. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "In most cases, actual SPH code implementations initially tar-get a specific simulation scenario (or a subset). This means that in many cases these hydrodynamics codes are implemented with the physics needed to solve a specific problem in mind, while the parallelization, scaling, and resilience take a relevant role only later. This importance shift (or rebalancing) becomes relevant when addressing much larger problems that require much more compu-tational resources, or when the computing infrastructures change or evolve. The philosophy behind the design of our SPH-EXA mini-app reflects the opposite. Knowing that the SPH technique is so ubiquitous, we developed the mini-app targeting the emerging Ex-ascale infrastructures. We took into account the state-of-the-art SPH methodology, learnt from current production SPH codes, and implemented it having in mind the design characteristics for per-formance that would be desirable in a code that could potentially reach sustained ExaFLOP/s, namely scalability, adaptability, and fault tolerance. To reach this goal, we employed state-of-the-art computer science methodologies, explored different parallelization paradigms and their combinations, and used solid software devel-opment practices. All this within a close multi-directional inter-disciplinary collaboration between scientists from fluid dynamics, astrophysics, cosmology, and computer science."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 220, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Lighter than production codes, mini-apps are algorithm-oriented and allow easy modifications and experiments [8]. Indeed, a single function can be evaluated using different strategies leading to dif-ferent performance results, even if the physical result is unchanged. These evaluation strategies may rely on vectorization, node level multi-threading, or cross-node parallelism. Their efficiency also depends on platform configuration: presence of accelerators, gen-eration of CPU, interconnection network fabric and topology, and others. Therefore, a mini-app is perfectly suitable as a portable code sandbox to optimize a numerical method, such as the SPH method, for Exascale."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 222, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PASC '20, June 29-July 1, 2020, Geneva, Switzerland"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The SPH-EXA mini-app is a C++14 lightweight and flexible header-only code with no external software dependencies, that works by default with double precision data types. Parallelism is ex-pressed via multiple programming models, which can be chosen at compilation time with or without accelerator support, for a hybrid node-core-accelerator configuration: MPI+OpenMP+OpenACC|OpenMP Target Offloading|CUDA. The SPH-EXA mini-app can be compiled with the GCC, Clang, PGI, Intel, and Cray (as long as the given com-piler supports the chosen programming model) C/C++ compilers."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The code is open-source and is freely available on GitHub under the MIT license at:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 288, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Weak-scaling exupercomputer have shown that the SPH-EXA mini-app can execute with up to 65 bil-lion particles1on 2,048 hybrid CPU-GPU nodes at 67% efficiency."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 288, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "These results are, therefore, very promising especially given that the efficiency of the code decreased very little when scaling from 512 to 2,048 nodes (see Section 5.2). Similar efficiency results are also reported in the strong-scaling results included in Figure 8."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 414, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "This paper is organized as follows. Section 2 presents a short overview of other representative mini-apps from scientific comput-ing and describes the differences compared to skeleton applications."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 288, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Section 3 is concentrated on the co-design aspects of this work."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Section 4 includes all the details related to the present implemen-tation of the SPH-EXA mini-app, including the implemented SPH version, the details on the employed parallel models and domain decomposition. Section 5 presents the results obtained regarding validation and verification, and the scaling experiments conducted."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 288, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Finally, Section 6 discusses the next steps for the project and Section 7 presents the conclusions."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 288, "firstLine": 4}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "RELATED WORK"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Mini-apps or proxy-apps have received great attention in recent years, with several projects being developed. The Mantevo Suite [9] devised at Sandia National Laboratory for high performance com-puting (HPC) is one of the first large mini-app sets. It includes mini-apps that represent the performance of finite-element codes, molecular dynamics, and contact detection, to name a few."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 288, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Another example is CGPOP [12], a mini-app from oceanogra-phy, that implements a conjugate gradient solver to represent the bottleneck of the full Parallel Ocean Program application. CGPOP is used for experimenting with new programming models and to ensure performance portability."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 288, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "At Los Alamos National Laboratory, MCMini [13] was devel-oped as a co-design application for Exascale research. MCMini implements Monte Carlo neutron transport in OpenCL and targets accelerators and coprocessor technologies."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 288, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The CESAR Proxy-apps [14] represent a collection of mini-apps belonging to three main fields: thermal hydraulics for fluid codes, neutronics for neutronics codes, and coupling and data analytics for data-intensive tasks."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 394, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The European ESCAPE project [15] defines and encapsulates the fundamental building blocks ('Weather & Climate Dwarfs') that underlie weather and climate services. This serves as a prerequisite"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 414, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1The largest cosmological SPH simulation to date performed with the parent codes is of the order of 25 billion particles [2]. Although, comparison is not fair due to different levels of physics included, it gives an order of magnitude reference."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 414, "hanging": 2}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Aur\u00e9lien Cavelan, Rub\u00e9n M. Cabez\u00f3n, Michal Grabarczyk, and Florina M. Ciorba"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 240, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "for any subsequent co-design, optimization, and adaptation efforts. One of the ESCAPE outcomes is Atlas [16], a library for numerical weather prediction and climate modeling, with the primary goal of exploiting the emerging hardware architectures forecasted to be available in the next few decades. Interoperability across the variegated solutions that the hardware landscape offers is a key factor for an efficient software and hardware co-design [17], thus of great importance when targeting Exascale systems."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 58, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In the context of parallel programming models, research has been focusing on the efficient use of intra-node parallelism, able to prop-erly exploit the underlying communication system through a fine grain task-based approach, ranging from libraries (Intel TBB [22]) to language extensions (Intel Cilk Plus [23] or OpenMP), to ex-perimental programming languages with focus on productivity (Chapel [24]). Kokkos [25] offers a programming model, in C++, to write portable applications for complex manycore architectures, aiming for performance portability. HPX [26] is a task-based asyn-chronous programming model that offers a solution for homoge-neous execution of remote and local operations."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 60, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Similar to these works, the creation of a mini-app directly from existing codes rather than the development of a code that mimics a class of algorithms has been recently discussed [18]. A scheme to follow was proposed therein that must be adapted according to the specific field the parent code originates in. To maximize the impact of a mini-app on the scientific community, it is important to keep the build and execution system simple, to not discourage potential users. The building should be kept as simple as a Makefile and the preparation of an execution run to a handful of command line arguments: \"if more than this level of complexity seems to be"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 66, "right": 50, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "required, it is possible that the resulting MiniApp itself is too complex to be human-parseable, reducing its usefulness.\" [18]."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 66, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The present work introduces the interdisciplinary co-design of an SPH-EXA mini-app with three parent SPH codes originating in the astrophysics academic community and the industrial CFD community. This represents a category not discussed in [18]."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 66, "right": 50, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "CO-DESIGN"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 66, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Being optimization critical to achieve the scalability needed to exploit Exascale computers, the long-term goal of the SPH-EXA [5] is to provide a parallel, optimized, state-of-the-art implementation of basic SPH operands with classical test cases used by the SPH community. This can be implemented at different levels: employing state-of-the art programming languages, dynamic load balancing algorithms, fault-tolerance techniques, and optimized tools and libraries."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 66, "right": 50, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Interdisciplinary co-design and co-development [9] allow to ad-equately involve the developers of the parent codes (in our case ChaNGa, SPH-flow, and SPHYNX), thus boosting and improving the design and the implementation of the SPH-EXA mini-app. We em-ploy an interdisciplinary co-design approach that goes beyond the classical hardware-software approach and holistically co-design be-tween the SPH application, the algorithms it employs (SPH method, distributed tree, finding neighbors, load balancing, silent data cor-ruption detection and recovery, optimal checkpointing intervals), and the HPC systems (via efficient parallelization with various clas-sical and modern programming models). To gauge the efficiency"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 66, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A Smoothed Particle Hydrodynamics Mini-App for Exascale"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "of additional modern programming paradigms (e.g., asynchronous tasking) against the classical ones for SPH, we also parallelize the SPH mini-app using classical paradigms, such as MPI, OpenMP, CUDA, and the more recent OpenMP target offloading, OpenACC, and HPX. This way, we cover all aspects that influence the perfor-mance and scalability of the SPH codes on modern and future HPC architectures."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In the first development stage, the focus has been on identifying and implementing the vanilla SPH solver (i.e. that with the original equations of [20]), the general workflow of which is shown in Fig-ure 1. The solver has been designed from scratch as a distributed application. To achieve maximum efficiency and scalability on cur-rent Petascale and upcoming Exascale systems, one of the main challenges is to minimize the inter-process communication and synchronization. In particular, global synchronizations are avoided as much as possible as this would result in all processes having to communicate with all others, resulting in global idleness and loss of efficiency. Instead, we focused on developing a method that fa-vors nearest neighbors communication between computing nodes, and only relies on collective communication for simple, yet neces-sary operations (e.g. computing the new size of the computational domain or the total energy). Currently, the SPH-EXA mini-app in-cludes a state-of-the-art implementation of the SPH equations (see Section. 4.1), which has been built progressively atop the optimal initial version."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "THE SPH-EXA MINI-APP"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The SPH-EXA mini-app is described in the following, together with its latest developments, supported physics features, and parallel pro-gramming models. The code is open-source and is freely available on GitHub under the MIT license at: ."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "SPH"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The SPH method has been implemented in the SPH-EXA mini-app following the formalism described in [1]. The main equations that express the calculation of the local density, and momentum and energy rates of change are:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 256, "hanging": 6}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03c1a =\ufffdmbWab(ha) , "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(1)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 378, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffddvi\ufffda = \u2212\ufffdmb\ufffd\u2126a\u03c12a"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 20, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Pa"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 3102, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Ai,ab(ha) + \u2126b\u03c12"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1580, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Pb"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1694, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "b"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1578, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Ai,ab(hb)\ufffd+ aAV i,a,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 114, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(2)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 140, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffddu\ufffda"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 84, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "=Pa"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 616, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2126a\u03c1a\ufffd\ufffdmb(vi,a \u2212 vi,b)Ai,ab(ha)+"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 760, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 754, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2\ufffd"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(vi,a \u2212 vi,b) aAV i,a, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(3)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 754, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "aAV i,a=1 2\ufffdmb\u03a0\u2032ab\ufffd Ai,ab(ha) +Ai,ab(hb)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03c1b"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "\ufffd, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(4)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 3116, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where subindex a is the particle index, b runs for its neighbors"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "indexes, and i is the spatial dimension index. \u03c1, m, P, and vi are the"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "density, mass, pressure, and velocity components of the particle,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "respectively. Wab is the SPH interpolation kernel, which depends"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PASC '20, June 29-July 1, 2020, Geneva, Switzerland"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 52, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 292, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 1: SPH general workflow. Computational steps are performed for every particle in the simulation domain over a number of time-steps. Point-to-point and collective com-munications update the particles information after certain computational steps. The complexity of each step is shown in red, where n is the total number of particles and m is the number of neighboring particles of each particle and de-pends on the smoothing length h."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PASC '20, June 29-July 1, 2020, Geneva, Switzerland"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "on the local spatial resolution h, named smoothing length. \u2126 are the grad-h terms that take into account the changes in the local smoothing length. Ai,ab(ha) are the terms for integral approach to derivatives, IAD, (see [1, 28] for more details), and aAV i "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "are the artificial viscosity acceleration components, where:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03a0\u2032ab=\ufffd\u2212\u03b1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "0 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "2vsi\u0434 abwab "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "for xab\u00b7vab < 0, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "otherwise, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(5)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 902, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "is the artificial viscosity disipation term. vsi\u0434 an estimate of the signal velocity between particles a,b, c is the ab= ca + cb \u2212 3wab is"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "local speed of sound, and wab = vab \u00b7 xab/|xab |. Finally, \u03c1\u22121 ab= 2 (\u03c1a + \u03c1b)\u22121. Updates to the position and velocity of the particles are done with a 2ndorder Press method, while energy is updated via a 2nd order Adams-Bashforth scheme."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The SPH-EXA mini-app currently implements the Sinc-family of interpolating kernels, the benefits of which are described in [27]:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Ws n(v,h,n) =\ufffd Bn hd Sn( \u03c0 2v) "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "for 0 \u2264 v \u2264 2, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "for v > 2, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(6)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 832, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "constant, and d the spatial dimension. The function sinc is defined where Sn(.) = [sinc(.)]n, n is a real exponent, Bn a normalization"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "widely used in spectral theory. as: sinc(\u03c0 2v) = sin( \u03c0 2v)/ \u03c0 2v, where v = |xa \u2212 xb |/ha, and it is"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Additional SPH kernels can be plugged in, upon implementation (in the above cases in only 6 LOC), proving how such a mini-app can be useful in allowing easy and quick modifications to experiment with alternative solutions. While not all SPH existing techniques and algorithms need to be implemented, a number of them, such as the SPH interpolation kernels, artificial viscosity treatments, or generalized volume elements, for example, can be later developed as separate interchangeable code modules. The SPH-EXA mini-app kernels have been optimized to enable excellent automatic vectorization via compiler optimizations."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 206}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Finally, as astrophysical and cosmological scenarios are within the scope of the SPH-EXA mini-app, we also implemented a multi-polar expansion algorithm to evaluate self-gravity using the same tree structure that we use to find neighboring particles and based on which we perform the domain decomposition (see Section 4.3 for more details on the latter)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "These components are common to many production SPH codes and represent the basis over which we can optimize and extend the functionality of the SPH-EXA mini-app."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 140, "firstLine": 200}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Parallel Software Development"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 112, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The SPH workflow illustrated in Figure 1 is implemented in code as shown in the sequence diagram included in Figure 2. To ensure code flexibility, extensibility, and readability we follow solid principles of the code design, use continuous integration to avoid regression, and name functions and classes appropriately. Additionally we defined coding standard for the project which can be applied automatically by the use of the clang-format tool."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 134, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Regarding core-level optimization, particles are kept in memory in an order that matches the octree, so that particles that are close to each other in the 3D space, are also close in memory to minimize"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 134, "firstLine": 200}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Aur\u00e9lien Cavelan, Rub\u00e9n M. Cabez\u00f3n, Michal Grabarczyk, and Florina M. Ciorba"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 402, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "cache misses. Additionally, widely used values are precomputed and stored either in memory or in lookup tables."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The goal for developing SPH-EXA mini-app using parallel pro-gramming is to provide a reference implementation in MPI+X. The MPI standard is the de facto communication library for distributed applications in HPC, due to the lack of an outperforming alternative for inter-node communication. OpenMP is the de facto standard for parallel multithreaded programming on shared-memory archi-tectures, widely used in academic, industry, and government labs. The hybrid MPI+OpenMP programming model represents a solid starting point for the parallel and distributed execution of the SPH-EXA mini-app. However, the vanilla MPI+OpenMP does not fully exploit the heterogeneous parallelism in the newest hardware ar-chitectures, Therefore, since version 4.5, the OpenMP standard [21] supports offloading of work to target accelerators. Other languages directly targeting accelerators have been proposed and accepted by the community, such as OpenACC (a directive-based programming model targeting a CPU+accelerator system, similar to OpenMP), CUDA (an explicit programming model for GPU accelerators) and OpenCL."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 222, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The SPH-EXA mini-app currently implements HPX as an exper-imental development branch to explore the efficiency of task-based models and potential on (pre-)Exascale machines. The aims of ex-ploring such task-based asynchronous programming model are to overlap computations and communications, both intra-node and inter-node, and to remove all synchronizations and barriers."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In terms of I/O from/to file system, the SPH-EXA mini-app has been designed from scratch to handle distributed data. The idea is that each computing node generates or loads a subset of the data. We currently support MPI I/O (in a branch) to perform parallel I/O operations at large scale by reading input and writing output binary data. We plan to move to HDF5 in the next months."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 218, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In terms of precision, round-off errors in single precision can add up to levels that might render the calculation useless, even using code units, mostly due to lack of accuracy in the evaluation of the gravitational forces using the tree. Nevertheless, this is based on the experience with the parent codes and we still have not studied this option in detail. Thus, we continue to use double precision data types while planning an exploration of a mixed-precision approach in future work."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 50, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The parallel program flow of the SPH-EXA mini-app for a typical test case (Rotating Square Patch [4]) is shown in Figure 2. The dia-gram describes the main execution loop, which is used to compute a time-step, and the sub-loops, where OpenMP and GPU offloading is used to accelerate the computation within single time-steps. Note that there are only three global synchronizations (MPI_AllReduce) across distributed-memory nodes. The first synchronization is used to count the number of tree nodes when performing domain de-composition (described later in \u00a74.3 and illustrated in Figure 3), the second synchronization is used to compute the minimum time-step (\u2206t) needed to advance the system in time, and the last synchro-nization is optional but useful for tracking total momentum and energy for verifying that they are conserved."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 222, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A Smoothed Particle Hydrodynamics Mini-App for Exascale "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "PASC '20, June 29-July 1, 2020, Geneva, Switzerland"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 2: Sequence diagram of the SPH-EXA mini-app illustrating its parallel program flow, the use of the various parallel"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "programming models, its complexity, and its synchronization points."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PASC '20, June 29-July 1, 2020, Geneva, Switzerland"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Domain Decomposition"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The spatial 3D domain of particles is decomposed into cells using an oct-tree data structure. The oct-tree is global - every node keeps an identical copy of it - and it is created only once at the beginning of the execution. The tree is then simply updated every iteration: branches are added or removed from the tree as needed. This global tree only contains general information such as the number of particles per cell, and does need to be refined down to single particles. Only the 'top' part of the tree is virtually shared and maintained identical by every computing node. Compared to existing methods, this approach only requires two collective communications (MPI_AllReduce): one to compute the number of particles in every cell of the tree and one to compute the new size of the spatial domain after particles have moved. Nevertheless, we aim at also circumventing these two collective communications, whose final objective is to rebuild the global tree to maintain particle load balance. Because particles can only move within the range of their local smoothing length, the upper levels of the global tree change at a much longer timescale than that of the particles dynamics. Therefore, global communications can be done scarcely while still avoiding particle load imbalance."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 144, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Domain cells (tree branches) are assigned to MPI processes us-ing the global tree to guarantee data locality, as shown in Fig-ure 3. The assignment process goes down to nodes that do not have particles fewer than the value globalBucketSize (a user-defined control parameter) which is typically equal to 128 particles. This way, certain processes can be assigned an additional tree node compared to others, but the difference in particles per process will be no more than the value assigned to globalBucketSize (as mentioned above). This causes imbalance equal to globalBucket-Size/noOfParticlesPerMpiRank * 100%, i.e., for 1 million particles per MPI rank and globalBucketSize of 128, the particle assignment imbalance will be equal to 0.01%."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In practice, the imbalance is higher because MPI ranks will have different numbers of halo particles, i.e. neighbors of an SPH particle located in the memory of another node need to be communicated and stored for the current iteration. Note that the amount of work does not increase (the number of neighbors remains the same, e.g. 300 per particle). However, more memory is required to store the additional particles. For example, the amount of halo particles for 300 neighbors varies by as much as 100% for a few thousand particles per MPI rank to 1-5% for a few million particles per MPI rank."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 232, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In addition, particle data is reordered in the local memory of every computing node to follow the depth-first-search ordering sequence of the oct-tree (similar to a Morton ordering sequence, or to using space filling curves, SFC, in general). This ensures that particles that are close together in the spatial domain are also close together in memory, resulting in increased cache efficiency. In terms of memory consumption, 1,459B are needed per particle at 1.35 GB per 1 Million particles and assuming 300 neighbors per particle. We use double precision floating point numbers for all physical properties and integers for storing particle's neighbor indices. Currently we store the indices of the neighbors for each particle."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 232, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Aur\u00e9lien Cavelan, Rub\u00e9n M. Cabez\u00f3n, Michal Grabarczyk, and Florina M. Ciorba"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 402, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 3: Domain decomposition for the Square Patch test case at time iteration 8,000 for 1M particles on 20 processes. Each color corresponds to a different MPI process."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.4 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Communication"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Particles are exchanged between computing nodes in every simula-tion time-step when they move from one sub-domain to another. Halo particles are exchanged thrice per-time-steps. The SPH-EXA mini-app relies on asynchronous point-to-point communications (MPI_Isend/MPI_Irecv) between nodes to avoid global synchro-nizations. Figure 4 shows the communication matrix of the SPH-EXA mini-app for an execution using 240 MPI processes. This il-lustration shows that processes only communicate with their close neighbors, reducing latency and contention compared with naive collective-communication based implementations, while nodes re-main synchronized with their neighbors in a loose fashion."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 222, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 4: Communication between individual processes for a 240 MPI processes execution of the SPH-EXA mini-app with sender ID on the Y-axis and receiver ID on the X-axis."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 220, "right": 254, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.5 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Hybrid Computing"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The SPH-EXA mini-app offloads the most compute-intensive ker-nels (density, IAD and momentum equations) to GPUs. It can be compiled with or without support for multi-processing via MPI, multi-threading via OpenMP, and acceleration via OpenMP 4.5, OpenACC or CUDA. It supports the following combinations:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 144, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PASC '20, June 29-July 1, 2020, Geneva, Switzerland"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "an incompressible Poisson equation and expressed as a rapidly converging series:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 144, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "P0 = \u03c1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 14, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "m=0\ufffd"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "mn\u03c02"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "\u221232\u03c92"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "\ufffd\ufffd m\u03c0\ufffd2 + \ufffd n\u03c0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 30, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd2\ufffd \u00d7sin\ufffdm\u03c0x"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "sin"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffdn\u03c0x"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": ","}], "style": {"indent": {"TYPE": "CT_Ind", "left": 36, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where \u03c1 is the density and L is the side length of the square. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "We also verified the results of the SPH-EXA mini-app against the outcome of simulating the same rotating square patch test, with the same initial conditions, in all three parent codes. To that extent we compared the total angular momentum\ufffdLtot at t = 0.5 s among all codes. The mini-app yields\ufffdLtot = 8.33 \u00d7 109g\u00b7cm2, which differs from the parent codes by \u223c 0.2%. This is well within the differences among the parent codes themselves. This small discrepancy comes from the different implementation of relevant sections of the SPH codes. Namely, the way of calculating gradients, volume elements, evaluate the new time-step, and integrate the movement equations. Nevertheless, despite these capital differences in implementation among the codes, the results converge well enough to ensure an adequate evolution of the system, pointing at the fact that the fundamentals of the SPH technique are correctly implemented in the SPH-EXA mini-app."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "EXPERIMENTS"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 126, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In this section, we report the results of a weak-scaling and a strong-scaling experiment conducted on a top production supercomputer. The immediate goal is to assess the performance of the SPH-EXA mini-app on a Petascale system comprising both CPUs and GPUs. The long-term goal is to run on Exascale systems. The weak-scaling efficiency is obtained as Tseq/Tpar \u00d7 100%. For strong-scaling, we plotted the average wall-clock time per iteration on a logarithmic scale. The average time per iteration is obtained from 10 iterations due to the limited number of node hours available for this project. However, a single run up to 8000 iterations used for validation has shown that the time per execution remains almost constant over 10 iterations with very small variations."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "We show that the proposed SPH-EXA mini-app shows promis-ing results, executing simulations with 65 billion particles on 2,048 hybrid CPU+GPU nodes at 67% weak-scaling efficiency. Moreover, the achieved weak-scaling efficiency decreased very little when scaling up from 512 to 2,048 nodes while keeping 32 million par-ticles/node. For strong-scaling, the average time per iteration de-creases almost linearly up to 1024 nodes for a fixed problem size of 6443= 267M particles. Memory consumption equals to 1,459B per particle, 1.35GB per 1 million particles."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In addition, an independent performance audit of the SPH-EXA mini-app has recently been performed by RWTH Aachen as part of the POP2 CoE service for European Scientific Applications. The report analyzed the efficiency and scalability of the SPH-EXA mini-app through a set of strong-scaling experiments with up to 960 MPI ranks (40 nodes) and showed that both are very high."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 200}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Aur\u00e9lien Cavelan, Rub\u00e9n M. Cabez\u00f3n, Michal Grabarczyk, and Florina M. Ciorba"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 402, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "System Overview"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The experiments were performed on the hybrid partition of the Piz Daint3supercomputer using PrgEnv-Intel, Cray MPICH 7.7.2 and OpenMP 4.0 (version/201611)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 50, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "We used the hybrid partition of more than 5,000 Cray XC50 nodes. These hybrid nodes are equipped with an Intel E5-2690 v3 CPU (codename Haswell, each with 12 CPU cores) and a PCIe version of the NVIDIA Tesla P100 GPU (Pascal architecture, 3584 CUDA cores) with 16 GB second generation high bandwidth memory (HBM2). The nodes of both partitions are interconnected in one fabric based on Aries technology in a Dragonfly topology4."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 220, "right": 0, "firstLine": 204}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Weak-Scaling Experiments"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 6 and Figure 7 show the results of a weak-scaling experiment conducted on the hybrid partition of the Piz Daint supercomputer. Both figures show the efficiency of the execution with increasing number of nodes relative to using a single computing node. The run performs the same 3D version of the CFD rotating square patch test described in \u00a74.6, with 32 million particles per computing node for a total of 65 billion particles on 2,048 nodes. The SPH-EXA mini-app shows very good parallel efficiency, namely 67% when running the largest test case. Note that the mini-app only experienced a 3% decrease in parallel efficiency when moving from 512 nodes to 2,048 nodes, i.e., increasing the number of MPI ranks and the problem size, by a factor of 4\u00d7."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "100"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 602, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "80"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 672, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Weak scaling efficiency (%)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "60"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 64, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "40"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 64, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "20"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 672, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Total Weak-Scaling Efficiency"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 2218, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 32, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "16"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "32"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "64"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "128"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "256"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "512"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1024 2048"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Number of computing nodes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1364, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 6: Weak scaling of the SPH-EXA mini-app with 32 million particles per node"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 7 shows the breakdown of efficiency by function. We observe that the decrease in efficiency is mostly due to the Domain Decomposition and Build Tree steps. Domain Decomposition is the most complex part of the code, responsible for distributing the particle data and performing load balancing. While the number of particles per process remains constant, the global tree - the top part of the tree that is kept identical on all processes - becomes larger,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 30, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "4f"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 720, "firstLine": 4}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}, {"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}, {"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A Smoothed Particle Hydrodynamics Mini-App for Exascale"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "100"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 380, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "80"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 450, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Weak scaling efficiency (%)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 242, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "60"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 60, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Domain Decomposition"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 270, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "40"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 60, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "20"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 60, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Build Tree"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 270, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Find Neighbors"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 270, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Density"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 270, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Equation of State (EOS)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 270, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Integral Approach to Derivative (IAD)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1120, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Momentum and Energy"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1120, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 28, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "16"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "32"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "64"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "128"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "256"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "512"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1024 2048"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 76, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Number of computing nodes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1742, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 7: Weak scaling per function of the SPH-EXA mini-app with 32 million particles per node"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 288, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "and this results in increased depth and an additional overhead as the number of nodes (and particles) increases globally. However, the depth of the tree increases with lo\u0434(n), where n is the total number of particles. This is reflected in Figure 7, orange and blue lines, where the efficiency decreases rapidly when increasing the node count from 1 to 64, but then remains stable and decreases very little (note the logarithmic scale on the X-axis). All other computing steps scale almost perfectly."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 408, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "These results are very encouraging and indicate that good scala-bility can be expected at much higher node counts. The SPH-EXA mini-app has not yet reached the tipping point where the code will not benefit from increased computing resources. If we assume the current efficiency trend, we can expect the code to run in excess of a trillion particles on a system consisting of 31,250 computing nodes with 32 million particles per nodes, which will be on par with future Exascale systems node counts."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 288, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Strong Scaling Experiments"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 8 shows the results of a strong-scaling experiment con-ducted with the SPH-EXA mini-app on 267 million particles on the hybrid partition of the Piz Daint supercomputer using up to 2,048 nodes. The results were obtained by running the mini-app blueusing MPI+OpenMP+CUDA. Additional dotted lines show the execution time per function of the mini-app."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 288, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "blueWhile the SPH-EXA mini-app has a sub-optimal speedup, it is clear that all functions benefit from the additional computational resources, and that the execution time continues to decrease even when using 2,048 nodes. In this strong-scaling scenario the total number of particles is fixed and nodes receive fewer particles as we increase the number of computing nodes, e.g., there are 4.1 million particles per node with 64 computing nodes, but only 130,000 per node (i.e. just above 10,000 per core) when using 2,048 computing nodes. However, particles maintain the same number of neighbors. This means the overlap between nodes increases and more halo particles are needed."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 288, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PASC '20, June 29-July 1, 2020, Geneva, Switzerland"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 398, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "102"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 446, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Strong-scaling execution time (s)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "101"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 114, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Total Execution Time"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 274, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "100"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 114, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Total Execution Time (ideal)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 274, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "10\u22121"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 36, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Domain Decomposition"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 274, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Build Tree"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 274, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Find Neighbors"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 274, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Density"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 274, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Equation of State (EOS)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 274, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Integral Approach to Derivative (IAD)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Momentum and Energy"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1166, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "10\u22122 "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "64"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 196, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "128"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "256"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "512"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1024"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2048"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Number of computing nodes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1712, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 8: Strong scaling of the SPH-EXA mini-app with 267 million particles"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 52, "right": 288, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "blueFigure 9 shows the mean and max ratio of halo particles per node, with respect to the number of particles initially assigned to a node, as described in Section 4.3. While only 30% of extra halos particles are needed on average using 64 nodes, more than 200% are needed when using 2,048 nodes."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 52, "right": 392, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "blueThe impact of having more halo particles per node is two-fold: (1) additional memory is required and (2) particles are more likely to be distant in memory space, which increases the number of cache-misses. Both aspects adversely impact performance. In a realistic setup, i.e., from 64 to 128 compute nodes in Figure 9, we aim at a few million particles per nodes and a few hundred thousand particles per core / thread. Such configurations also minimize the number of halo particles per node."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 52, "right": 288, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Mean Halo Ratio"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Max Halo Ratio"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "400"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 426, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Halo particles ratio (%)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "300"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 34, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "200"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 34, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "100"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 426, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "64"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "128"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "256"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "512"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1024"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2048"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Number of computing nodes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1712, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 9: Mean and max ratio of halo particles per comput-ing node w.r.t. initially assigned particles per node, for the test case with 267 million particles."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 52, "right": 288, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PASC '20, June 29-July 1, 2020, Geneva, Switzerland"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.4 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Current Limitations"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The weak-scaling experiment used 100% of the available memory on every computing node and on every GPU. The execution time for a single time-step remained, on average, well under a minute. Hence, a high priority optimization is to reduce the memory footprint of the SPH-EXA mini-app, which will come at the expense of longer execution times but will allow us to address and execute larger problem sizes in the future."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 232, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Our current project allocation on Piz Daint did not allow us to run larger experiments at the time of writing. We are currently working on obtaining larger allocations to launch simulations using the full system."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 246, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "6 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "NEXT STEPS AND FUTURE WORK"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The mini-app provides a modular tool that can be used by the community to plug-in additional physical processes or numerical methods. As there is a wide variety of very demanding physical scenarios that will be targeted by the SPH-EXA mini-app (e.g. su-pernova explosions, galaxy formation), having a highly scalable and modular code will encourage others in the community to contribute with their own physics/numerics modules."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 144, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Our short-term objectives include reducing the memory foot-print of the SPH-EXA mini-app, which will help increasing the par-ticle count per node beyond 32 million particles, adding additional features such as the possibility to select the desired generalized volume elements, and simulating increasingly complex scenarios. Regarding fault tolerance, we plan to include our novel detection method for silent data corruption [29], which is specifically de-signed for SPH applications, add automatic validation (through conserved quantities), and implement checkpointing at optimal intervals. Each new feature in the SPH-EXA mini-app will be it-eratively tested, validated, and optimized for efficiency from the start."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "As mid-term targets we aim to overlap computations with inter-node communications. Therefore, we will prepare the SPH-EXA mini-app to be Asynchronous Multi-Tasking (AMT) ready. For this, we will enable the opportunity to compare different tasking frame-works such as OpenMP and HPX. Moreover, this will also open the opportunity to delegate independent tasks to accelerators and CPUs at the same time, which, in terms of the increasing hardware heterogeneity foreseen in the pre-exascale and exascale systems, is crucial to achieve maximum load balance and performance. Ad-ditionally, we will employ multilevel (batch, process, and thread) scheduling for dynamic load balancing in the mini-app as a con-figurable option. This will allow us to systematically explore the interplay between load balancing at these levels to achieve the best possible load balancing during execution."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In the end, the SPH-EXA [5] follows a long-term vision, having set out to have major impact in the scientific communities it gath-ers (and beyond in the longer run). The aim of SPH-EXA [5] is to reach the capabilities of present HPC systems and to push those of the future HPC infrastructures for simulating the most complex phenomena at the highest resolution and longest physical times, such as exploring the explosion mechanisms of Type Ia and Core Collapse Supernovae, including nuclear reaction treatments via efficient nuclear networks and neutrino interactions with detailed"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Aur\u00e9lien Cavelan, Rub\u00e9n M. Cabez\u00f3n, Michal Grabarczyk, and Florina M. Ciorba"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 402, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "transport, respectively. Another target for the SPH-EXA mini-app is modeling the small-scale fluid-dynamics processes involved in the assembly of the planetary building blocks while capturing simulta-neously the large scale dynamics of the proto-planetary disks, and being able to simulate an entire population of galaxies from high to low redshift in a cosmological volume with enough resolution to model directly individual star forming regions. Hence, a flexible and modular code that scales efficiently and robustly on a large number of nodes nodes, accessing a mix of CPUs, GPUs, FPGAs, etc, is our long-term objective."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "7 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "CONCLUSION"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "SPH-EXA [5] is an interdisciplinary project involving Computer Scientists, Astrophysicists, and Cosmologists, brings together state-of-the-art methods in both fields under a broad scope, and addresses important challenges at all levels of existing and emerging infras-tructures by exploring novel programming paradigms and their combinations."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In this work, we described the current status of a novel and scalable SPH-EXA mini-app for simulating the SPH method on large hybrid HPC systems efficiently utilizing both multi-core CPUs and GPUs. The SPH-EXA mini-app is open source and has no external dependencies. With fewer than 3,000 modern C++ LOC (with no compromise for performance), it is easy to run, understand, modify, and extend. The code is simple by design, making it easy to test and implement new SPH kernels or other performance optimizations. We performed an initial exploration of the efficiency of different combinations of hybrid CPU and GPU programing models, with different compilers, verified and validated the SPH-EXA mini-app via computationally-demanding simulations, and compared the results with those obtained by the parent codes. We also conducted a weak-scaling experiment that shows excellent scaling, with 67% efficiency on 2,048 hybrid nodes of the Piz Daint supercomputer with a loss of just 3% in efficiency when going from 512 to 2,048 nodes."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 218, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "At this initial stage of the project we are already in a good po-sition to explore the limits of what can be done on current top supercomputers. This opens the doors to great number of poten-tial applications, not only of the SPH-EXA mini-app, but also of the learned lessons. The SPH-EXA mini-app has driven substan-tial improvements to its three parent codes (SPHYNX, ChaNGa, and SPH-flow) and has set a multi-directional knowledge transfer between the Computer Science, Astrophysics, and Computational Fluid Dynamics communities."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACKNOWLEDGEMENTS"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "This work is supported in part by the Swiss Platform for Advanced Scientific Computing (PASC) project SPH-EXA [5] (2017- 2021). The authors acknowledge the support of the Swiss National Su-percomputing Centre (CSCS) via allocation project c16, where the calculations have been performed. Several performance results were provided by the Performance Optimisation and Productivity (POP) centre of excellence in HPC."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "REFERENCES "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[1] R. M. Cabez\u00f3n, D. Garc\u00eda-Senz, and J. Figueira, SPHYNX: An accurate density-based "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "SPH method for astrophysical applications, A&A, 606:A78, Oct. 2017."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A Smoothed Particle Hydrodynamics Mini-App for Exascale"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[2] M. Tremmel, M. Karcher, F. Governato, M. Volonteri, T. R. Quinn, A. Pontzen, L. Anderson, and J. Bellovary, The Romulus cosmological simulations: a physical approach to the formation, dynamics and accretion models of SMBHs, MNRAS, 470, Sep. 2017."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 236, "hanging": 244}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[3] G. Oger, D. Le Touz\u00e9, D. Guibert, M. de Leffe, J. Biddiscombe, J. Soumagne, J-G."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Piccinali, On distributed memory MPI-based parallelization of SPH codes in massive HPC context, Computer Physics Communications, 200:1-14, March 2016."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[4] A. Colagrossi, A meshless Lagrangian method for free-surface and interface flows "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "with fragmentation, PhD thesis, Universit\u00e0 di Roma, 2005."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[5] Florina M. Ciorba, Lucio Mayer, Rub\u00e9n M. Cabez\u00f3n, and David Imbert, SPH-EXA: Optimizing Smoothed Particle Hydrodynamics for Exascale Computing, www.pasc-ch.org/projects/2017-2020/sph-exa/."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 238, "hanging": 244}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[6] M. Liebend\u00f6rfer, M. Rampp, H.-Th. Janka, and A. Mezzacappa, Supernova Simu-lations with Boltzmann Neutrino Transport: A Comparison of Methods, The Astro-physical Journal, Volume 620, Number 2, 2005."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 228, "hanging": 244}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[7] O. Agertz, B. Moore, J. Stadel, D. Potter, F. Miniati, J. Read, L. Mayer, A."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Gawryszczak, A. Kravtsov, A. Nordlund, F. Pearce, V. Quilis, D. Rudd, V. Springel, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "J. Stone, E. Tasker, R. Teyssier, J. Wadsley, and R. Walder, Fundamental differences "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "between SPH and grid methods. Monthly Notices of the Royal Astronomical Society, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "380: 963-978. doi:10.1111/j.1365-2966.2007.12183.x "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[8] R. F. Barret, C. T. Vaughan, and M. A. Heroux. MiniGhost: A mini-app for exploring"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "boundary exchange strategies using stencil computations in scientific parallel com-puting. Technical report No. SAND2012-2437, Sandia National Laboratories, 2012. www.sandia.gov/~rfbarre/PAPERS/MG-SAND2012-2437.pdf."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 242, "right": 230, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[9] M. A. Heroux, D. W. Doerfler, P. S. Crozier, J. M. Willenbring, H. C. Edwards, A."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Williams, M. Rajan, E. R. Keiter, H. K. Thornquist, and R. W. Numrich. Improving performance via mini-applications. Technical report No. SAND2009-5574, Sandia National Laboratories, 2009. mantevo.org/MantevoOverview.pdf."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 252, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[10] E. J. Tasker, R. Brunino, N. L. Mitchell, D. Michielsen, S. Hopton, F. R. Pearce, G."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "L. Bryan, and T. Theuns, A test suite for quantitative comparison of hydrodynamic "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "codes in astrophysics. Monthly Notices of the Royal Astronomical Society, 390: "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "1267-1281. doi:10.1111/j.1365-2966.2008.13836.x "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[11] R. M. Cabez\u00f3n, K-C. Pan, M. Liebend\u00f6rfer, T. Kuroda, K. Ebinger, O. Heinimann, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "F-K. Thielemann, and A. Perego, Core-collapse supernovae in the hall of mirrors. A "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "three-dimensional code-comparison project. Astron.Astrophys. 619 (2018) A118. [12] A. Stone, J. M. Dennis, M. Mills Strout. The CGPOP Miniapp, version 1.0. Technical "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Report CS-11-103, Colorado State University, 2011."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[13] R. Marcus. MCMini: Monte Carlo on GPGPU. Technical Report LA-UR-12-23206, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Los Alamos National Laboratory, 2012."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[14] T. C. Team. The CESAR codesign center: Early results. Technical report, Argonne "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "National Laboratory, 2012."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[15] P. Bauer, N. Wedi, and W. Deconinck. ESCAPE: Energy-efficient scalable algorithms "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "for weather prediction at Exascale. EU Horizon 2020."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[16] W. Deconinck, P. Bauer, M. Diamantakis, M. Hamrud, C. K\u00fchnlein, P. Maciel, G."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Mengaldo, T. Quintino, B. Raoult, P. K. Smolarkiewicz, and N. P. Wedi. Atlas: A library for numerical weather prediction and climate modelling. Computer Phys. Comm., 220:188 - 204, 2017."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[17] T. C. Schulthess. Programming revisited. Nature Physics, 11(5):369-373, may 2015. [18] O. Messer, E. D'Azevedo, J. Hill, W. Joubert, S. Laosooksathit, and A. Tharrington."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Developing MiniApps on modern platforms using multiple programming models. In 2015 IEEE International Conference on Cluster Computing. IEEE, sep 2015."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[19] S. Rosswog, Astrophysical smooth particle hydrodynamics, New Astron-omy Reviews, Volume 53, Issues 4-6, 2009, Pages 78-104, ISSN 1387-6473, https://doi.org/10.1016/j.newar.2009.08.007."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 228, "hanging": 244}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[20] J. J. Monaghan, and R. A. Gringold, Shock Simulation by the Particle Method SPH, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Journal of Computational Physics, Volume 52, Issue 2, p. 374-389, 1983."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[21] OpenMP 4.5 Specifications., 2018. [22] J. Reinders. Intel Threadingcore Processor "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Parallelism. O'Reilly Media, 2007. ISBN:9780596514808."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[23] A. D. Robison, Composable Parallel Patterns with Intel Cilk Plus. Computing in "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Science & Engineering, vol. 15, no. 2, pp. 66-71, March-April 2013."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[24] B. L. Chamberlain et al., Parallel Programmability and the Chapel Language. The International Journal of High Performance Computing Applications, vol. 21, no. 3, Aug. 2007, pp. 291-312."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 236, "hanging": 244}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[25] H. Carter Edwards, C. R. Trott and D. Sunderland, Kokkos. Journal of Parallel and "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Distributed Computing, vol. 74, no. 12, Dec. 2014, pp. 3202-3216."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[26] H. Kaiser, T. Heller, B. Adelstein-Lelbach, A. Serio and D. Fey. HPX: A Task Based Programming Model in a Global Address Space. In Proceedings of the 8th International Conference on Partitioned Global Address Space Programming Models (PGAS '14). ACM, New York, NY, USA, 2014."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 252, "hanging": 244}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[27] R. M. Cabez\u00f3n, D. Garcia-Senz, A. Rela\u00f1o, A one-parameter family of interpolating kernels for Smoothed Particle Hydrodynamics studies, J. Comput. Phys., 227:8523-8540, 2008."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 144, "hanging": 244}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[28] D. Garc\u00eda-Senz, R. M. Cabez\u00f3n and J. A. Escart\u00edn, Improving smoothed particle hydrodynamics with an integral approach to calculating gradients, A&A, 538, A9, February 2012."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 236, "hanging": 244}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PASC '20, June 29-July 1, 2020, Geneva, Switzerland"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 44, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[29] Aur\u00e9lien Cavelan, Rub\u00e9n M. Cabez\u00f3n and Florina M. Ciorba, Detection of Silent"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Data Corruptions in Smoothed Particle Hydrodynamics, 19th IEEE/ACM Interna-"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 470, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "tional Symposium on Cluster, Cloud and Grid Computing, CCGRID 2019, Larnaca,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 470, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Cyprus, May 14-17, 2019."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 470, "right": 0, "firstLine": 0}}}]}]}
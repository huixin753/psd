{"TYPE": "document", "VALUE": [{"TYPE": "body", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "High Performance MPI over the Slingshot Interconnect: Early Experiences"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 288, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Kawthar Shafie Khorassani The Ohio State University "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Columbus, USA "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "shafiekhorassani.1@osu.edu"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Chen-Chun Chen "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "The Ohio State University Columbus, USA "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "chen.10252@osu.edu"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 432, "right": 576, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Bharath Ramesh "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "The Ohio State University Columbus, USA "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "ramesh.113@osu.edu"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 432, "right": 288, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Aamir Shafi "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "The Ohio State University Columbus, USA "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "shafi.16@osu.edu"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 432, "right": 576, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Hari Subramoni "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "The Ohio State University "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Columbus, USA "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "subramon@cse.ohio-state.edu"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 432, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Dhabaleswar K. Panda The Ohio State University Columbus, USA "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "panda@cse.ohio-state.edu"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 432, "right": 288, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ABSTRACT"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The Slingshot interconnect designed by HPE/Cray is becoming more relevant in High-Performance Computing with its deploy-ment on the upcoming exascale systems. In particular, it is the interconnect empowering the first exascale and highest-ranked supercomputer in the world, Frontier. It offers various features such as adaptive routing, congestion control, and isolated workloads. The deployment of newer interconnects raises questions about per-formance, scalability, and any potential bottlenecks as they are a critical element contributing to the scalability across nodes on these systems. In this paper, we will delve into the challenges the slingshot interconnect poses with current state-of-the-art MPI li-braries. In particular, we look at the scalability performance when using slingshot across nodes. We present a comprehensive eval-uation using various MPI and communication libraries including Cray MPICH, OpenMPI + UCX, RCCL, and MVAPICH2-GDR on GPUs on the Spock system, an early access cluster deployed with Slingshot and AMD MI100 GPUs, to emulate the Frontier system."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "KEYWORDS"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Slingshot, AMD GPUs, Interconnect Technology, MPI."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Reference Format: "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Kawthar Shafie Khorassani, Chen-Chun Chen, Bharath Ramesh, Aamir Shafi, Hari Subramoni, and Dhabaleswar K. Panda. 2022. High Performance MPI over the Slingshot Interconnect: Early Experiences. In Practice and"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Experience in Advanced Research Computing (PEARC '22), July 10-14, 2022, Boston, MA, USA. ACM, New York, NY, USA, 7 pages."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "INTRODUCTION"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 126, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The Frontier Supercomputer [7] deployed at the Oakridge Leader-ship Computing Facility (OLCF), now leading the Top500 [5] list of supercomputers in the world and officially recognized as the"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 236, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PEARC '22, July 10-14, 2022, Boston, MA, USA\u00a9 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-9161-0/22/07...$15.00"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 2304, "firstLine": 10}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "first exascale supercomputer, is empowered by the HPE Cray Sling-shot Interconnect. In preparation for the vast demands of exascale computing and moving to a slingshot-based networking environ-ment, it is important to have an understanding of the interconnect with respect to MPI communication. MPI libraries have been heav-ily deployed and used on systems with an underlying InfiniBand interconnect connecting nodes. They have been optimized and extensively researched in this ecosystem. Now, with upcoming ex-ascale systems choosing to deploy the Slingshot interconnect as the underlying connection between nodes, it is crucial to have an understanding of the interconnect technology and how it impacts or improves the performance of communication at scale [11], [16]."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In this paper, we provide an analysis of the performance of various MPI libraries on a system with preliminary/experimental deployment of the Slingshot Interconnect. As this is a new area that has seldom been researched and is going to become a critical component of future HPC deployment, it is important to have this kind of detailed information and analysis that could provide a better outlook on the needs for optimizations and enhancements on these systems. This drives future research and innovations while also providing scalable and competitive options in this ecosystem that compare or improve upon existing innovations in the current interconnect technology realms."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 222, "right": 50, "firstLine": 204}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Motivation"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Many of the top supercomputers [5] utilize InfiniBand network-ing, with the deployment of the Mellanox InfiniBand Interconnect to connect nodes across the network. This area has been heavily evaluated and analyzed over the years with various MPI libraries utilizing GPU-aware and CPU-based communication to scale out performance onto multiple nodes. This understanding of the limita-tions and advantages of the interconnect technology drove future directions in research over the years related to communication op-timization and performance analysis. With the deployment of the Slingshot interconnect, it is just as important to develop an under-standing of the advantages and features the interconnect introduces in order to motivate future approaches in the communication realm."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The underlying interconnect technology is a critical component in achieving high performance, low latency and high throughput, at scale on next-generation exascale systems. This drives the moti-vation to have a detailed analysis and understanding of the existing MPI libraries and the performance they are able to demonstrate at"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 222, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PEARC '22, July 10-14, 2022, Boston, MA, USA "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Shafie Khorassani, et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "certain scales, various configurations, and for different communi-cation operations. Through this work, we demonstrate a need for a thorough evaluation of communication over the newer Slingshot Interconnect and its ecosystem in preparation for exascale systems in order to achieve the scalability and efficiency that is promised by the next generation of supercomputing."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Key Insights and Contributions"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The performance of GPU-aware approaches to communication provided by the state-of-the-art communication libraries on the Slingshot interconnect have yet to be explored. There is a lack of thorough evaluation and analysis of performance comparing the different communication operations and detailing the demands for MPI at the application layer on a system with Slingshot Intercon-nects. Additionally, the system used in this study includes AMD MI100 GPUs, which are also a snapshot of the type of system and ecosystem we can expect for the next-generation exascale systems."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Through this work, we make the following contributions:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022 Comprehensive evaluation of GPU-aware communication using various communication libraries, including OpenMPI + UCX, MVAPICH2-GDR, Cray MPICH, and RCCL on the Spock system with the Slingshot-10 interconnect, AMD MI100 GPUs, and AMD EPYC Rome CPUs for point-to-point and collective benchmarks."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 252, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022 Application-level evaluation using state-of-the-art communi-cation libraries for rocHPCG and for the heFFTe application "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "using the rocfft backend for AMD GPUs."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 326, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022 Discuss the challenges that the current Slingshot-10 Intercon-nect brings about in terms of communication performance and what challenges to consider for future deployment of MPI libraries on systems with the upcoming Slingshot-11 Interconnect, in preparation for new exascale systems such as Frontier."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 494, "right": 220, "hanging": 168}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "BACKGROUND"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "State-of-the-art Interconnect Technologies"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Achieving high performance for complex HPC workloads that ben-efit from high levels of parallelism requires efficient and scalable network interconnects. Modern interconnects such as InfiniBand, RoCE, Omni-Path, etc., were introduced into the market to address communication bottlenecks by achieving low latency and high throughput between nodes. In recent years, InfiniBand and high-speed Ethernet represent the gold standard for high-performance network interconnects. For instance, Summit@ORNL (ranked 4th on the June 2022 Top500 list [5]), uses Dual-rail Mellanox EDR InfiniBand as the underlying interconnect. Approximately 35% of supercomputers in the Top500 utilize InfiniBand networking (in-cluding Sierra@LLNL, Selene@NVIDIA, etc.), and about 48% deploy Gigabit Ethernet networking (including Perlmutter@NERSC, Po-laris@ANL, etc). The adoption rates for interconnects in upcoming exascale systems are rapidly changing due to an increased number of choices and evolving interconnect standards."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 144, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Slingshot Interconnect"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "HPE Slingshot [11] is a high-performance network designed by HPE Cray for upcoming exascale-era systems, and is based on Eth-ernet. It provides flexibility and capabilities to enable users to run a wide mix of workflows. The switches support a high-radix and up to 12.8Tb/s bandwidth. While the latency of Ethernet networks is slightly worse when compared to InfiniBand systems in gen-eral, Ethernet networks claim the advantage of wider adoption across application domains. HPE Slingshot delivers low latency and high throughput for HPC workloads, and minimizes the number of switch hops in large networks (for instance, by employing the use of the Dragonfly [12] topology). The interconnect features adap-tive routing techniques to help maintain the balanced traffic flows through fine-grained optimization. HPE Slingshot also introduces a fully automatic and hardware-implemented congestion control mechanism to minimize the impact of congestion when multiple workloads run at the same time. It is currently empowering the first official exascale supercomputer in the world, Frontier@OLCF, and in the works to be deployed on future exascale supercomputers as well, such as El Capitan@LLNL."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 220, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2.3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "State-of-the-art Communication Libraries"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The Message Passing Interface (MPI) is a multi-processing para-digm that enables communication among processes on parallel architectures. The communication primitives can be categorized as one-sided, point-to-point, and collective operations. One-sided communication indicates the use of only one process to move data to a remote process (without the remote process's involvement). Hence, it's also referred to as remote memory access (RMA). It de-couples the process synchronization during data transfer. MPI_Put, MPI_Get, and MPI_Accumulate are well-known one-sided commu-nication operations. The MPI standard also supports expressing point-to-point communication operations using two-sided seman-tics using MPI_Send, MPI_Recv, MPI_Isend, and MPI_Irecv. Collec-tive communication operations defined by the MPI standard provide convenient abstractions for multiple processes/threads to efficiently communicate with one another. These operations can involve com-puting operations (in reduction collectives such as MPI_Allreduce and MPI_Reduce) or just communication to represent common patterns such as a broadcast, scatter, gather, and others."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Aside from the MPI interface, there are other communication libraries that use and expose a different underlying API to transfer messages. For example, the NVIDIA Collective Communication Library (NCCL), provides optimized communication primitives for GPU to GPU communication within as well as across the node for NVIDIA GPUs. ROCm Communication Collectives Library (RCCL) is the communication library based on NCCL for AMD GPUs, pro-viding primitives that enable GPU to GPU communication on AMD ROCm supported systems, similar to what NCCL achieves on sys-tems with NVIDIA GPUs."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 222, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2.4 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Limitations of State-of-the-art Approaches"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Existing MPI libraries provide support for various network fea-tures such as Omni-Path, RoCE, InfiniBand, etc. With the expected growth in deployment of the Slingshot Interconnect across up-coming systems, this will be added to the growing list of features"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "High Performance MPI over the Slingshot Interconnect: Early Experiences"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "that MPI libraries will need to add functionality and optimizations for. HPE designed the Slingshot Interconnect in such a way to be ethernet compatible in order to provide ease of interoperability with existing systems. This enables a direct connection between the switches for Slingshot and ethernet networks and storage de-vices [11]. It also provides support for features such as adaptive routing, congestion control, and isolated workloads. These fea-tures provide several challenges and possibilities to explore and enhance state-of-the-art communication libraries. The limitations of current state-of-the-art approaches will be made more clear with the deployment of Slingshot-11. Current accessibility and de-ployment on early access Slingshot systems provide an ecosystem with Slingshot-10 interconnection amongst nodes. The second gen-eration of Slingshot, Slingshot-11, is deployed over a Slingshot fabric and adapter, while the current deployment of Slingshot-10 is running over a Slingshot Network with a Mellanox InfiniBand adapter. This second-generation deployment introduces additional challenges for communication libraries to develop functionality over the underlying adapter and fabrics."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "EVALUATION AND ANALYSIS"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In this section, we provide details of the Spock system (Figure 1) used for the experiments and evaluations and the software envi-ronment on this system. We also provide additional details specific to the MPI and communication libraries used in the evaluation. We include a detailed analysis of communication performance using various MPI libraries at the benchmark and application layers."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "System and Software Details"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The performance evaluation is done on the Spock system deployed at the Oakridge Leadership Computing Facility(OLCF) [15]. This is an early access system provided in preparation for the exascale system, Frontier [7]. This preparation for the deployment of exas-cale systems allows for experiments and evaluations to be done in order to develop an understanding of what to expect in terms of communication library performance on the upcoming exascale sys-tems, and the challenges in relation to communication on a system with Slingshot Interconnects and the latest AMD GPUs."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 144, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 1: Spock System Details and Usage"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 772, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Software"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Version"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Reference"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MPI "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "& "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Communication Libraries"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Open MPI"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.1.4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[10]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "UCX"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1.12.1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[6]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Cray MPICH"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "8.1.14"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[19]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "RCCL"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.0.2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[4]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MVAPICH2-GDR"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2.3.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[17]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Platform"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ROCm"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.0.2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[2]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Benchmarks & "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Applications"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "OSU "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Micro-benchmarks"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.9"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[8]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "heFFTe"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2.0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[1]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The Spock cluster consists of 64-core AMD EPYC 7662 Rome CPUs, and 4 AMD MI100 GPUs with 32 GB HBM2 per node. The GPUs are connected within a node via Infinity Fabric and con-nected to the CPU via PCIe Gen4. The nodes are connected via the"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PEARC '22, July 10-14, 2022, Boston, MA, USA"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 52, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Slingshot-10 interconnect, providing 12.5 GB/s bandwidth across nodes. The latest version of ROCm deployed on the system is ROCm 5.0.2. This information is detailed in the Spock compute node pre-sented in Figure 1. More details of the communication libraries and software stack versions used on this system for this evaluation are provided in Table 1."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.1.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "MPI Libraries -. Table 2 details the various MPI libraries used and configuration details specific to each of the libraries. The MVAPICH2-GDR library v2.3.7 was used for the evaluations done on GPUs (MVAPICH2-GDR optimized for GPU-aware communi-cation). This library provides downloadable options from the site or through the user forum in order to execute on the system. Spe-cific configuration was not required here. The MVAPICH2-GDR installation is linked to ROCm 5.0.2, the latest version of ROCm on the Spock system. OpenMPI version 4.1.4 and UCX version 1.12.1, the latest versions of the stack were used in the performance evaluation. The configuration details of UCX to link with ROCm and enable optimizations and the details for linking OpenMPI to this UCX installation are demonstrated in the table. Cray MPICH 8.1.14 is the MPI library deployed on the Spock system by default. It required a load of the existing module, adding ROCm into the path, and loading an additional module to detect the architecture. These modules are detailed in the table below. Finally, the ROCm Collectives Communication Library (RCCL) was used as well in the evaluation of GPU-aware communication."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 222, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 2: MPI Libraries Configuration and Installation De-tails"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "hanging": 6}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Communication Libraries"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Configuration & Installation Details"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 576, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MVAPICH2-GDR 2.3.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MVAPICH2-GDR 2.3.7 + ROCm 5.0.2 for GPUs "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Run: MV2_USE_ROCM=1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 100, "right": 576, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "OpenMPI 4.1.4 + UCX 1.12.1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "UCX: --with-rocm=<path-to-rocm>--without-knem --without-cuda"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "--enable-optimizations "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "OpenMPI: --with-ucx=<path-to-ucx>--without-verbs "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Run: -x UCX_RNDV_THRESH=128"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 100, "right": 288, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Cray MPICH 8.1.14"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "module load craype-accel-amd-gfx908 module load cray-mpich/8.1.14 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Run: MPICH_GPU_SUPPORT_ENABLED=1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 100, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "RCCL 5.0.2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "CXX=<path-to-rocm>/bin/hipcc"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 100, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "OSU Micro-Benchmarks"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "To compare the performance of various communication operations on the Spock cluster using different MPI libraries, we utilize the OSU Micro-Benchmarks (OMB) suite version 5.9. It reports intra-and inter-node point-to-point latency and bandwidth, and the per-formance of MPI collective operations at different message sizes."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Micro-Benchmark Evaluation on GPUs"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In this section, we delve into the GPU-based evaluation utilizing GPU-aware MPI and communication libraries. We evaluate the"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PEARC '22, July 10-14, 2022, Boston, MA, USA "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Shafie Khorassani, et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 246, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 246, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 1: Spock Compute Node Details (Courtesy [16])"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 2908, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 112, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(a) Small Message Point-to-Point Latency"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(b) Large Message Point-to-Point Latency"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(c) Large Message Bandwidth"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 466, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(d) Large Message Bi-Directional Bandwidth"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 2: Intra-Node Point-to-Point Performance on GPUs over Infinity Fabric"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2126, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 112, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(a) Small Message Point-to-Point Latency"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(b) Large Message Point-to-Point Latency"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(c) Large Message Bandwidth"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 466, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(d) Large Message Bi-Directional Bandwidth"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 3: Inter-Node Point-to-Point Performance on GPUs over Slingshot-10 Interconnect"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1664, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "point-to-point performance of communication between two GPUs within the same node on the same socket, and two GPUs across nodes connected by the Slingshot-10 interconnect over the network. We also evaluate the performance of collective communication on the Spock system on up to 64 GPUs (16 Nodes with 4 GPUs per node)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 238, "right": 144, "firstLine": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.3.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Intra-Node Point-to-Point -. In Figure 2, we present an eval-uation of intra-node point-to-point benchmark-level performance comparing MVAPICH2-GDR, OpenMPI + UCX, and Cray MPICH on AMD MI100 GPUs. The evaluation is done between two GPUs within one node for latency (osu_latency), bandwidth (osu_bw), and bi-directional bandwidth (osu_bibw). For small message latency shown in Figure 2(a), MVAPICH2-GDR, OpenMPI + UCX, and Cray MPICH achieve 2.01 us, 3.79 us, and 2.44 us latency, respectively. This configuration involves two AMD MI100 GPUs within the same node, on the same socket, connected by Infinity Fabric. The trends in performance for intra-node communication between GPUs here reflects on protocols typically used for this configuration within"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 240, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MPI libraries such as: a GPU memory copy that utilizes the LargeBar feature of AMD GPUs and the ROCm driver for small message sizes, and ROCm IPC for larger message sizes [18]. The Infinity Fabric connection provides (46 + 46 GB/s) peak bandwidth. In Figure 2(c), MVAPICH2-GDR achieves a peak bandwidth at 1MB of 52 GB/s, OpenMPI + UCX achieving 30 GB/s, and Cray MPICH at 88 GB/s."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 30, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.3.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Inter-Node Point-to-Point -. In Figure 3 we present an eval-uation of inter-node point-to-point benchmark-level performance comparing MVAPICH2-GDR, OpenMPI + UCX, and Cray MPICH on AMD MI100 GPUs. The evaluation is done between two GPUs on two different nodes connected by the Slingshot-10 interconnect for latency (osu_latency), bandwidth (osu_bw), and bi-directional bandwidth (osu_bibw). In Figure 3(a) and Figure 3(b), we see that MVAPICH2-GDR and Cray MPICH achieve 3.73 us and 3.8 us latency at 4B and 115.26 us and 148.08 us at 1MB, respectively. With this configuration over the Slingshot-10 interconnect, with 12.5GB/s peak achievable bandwidth, MVAPICH2-GDR has peak"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 218, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "High Performance MPI over the Slingshot Interconnect: Early Experiences"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 246, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PEARC '22, July 10-14, 2022, Boston, MA, USA"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 90, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(a) REDUCE - Small Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(b) REDUCE - Large Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(c) ALLREDUCE - Small Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(d) ALREDUCE - Large Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 206, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 4: Performance of MPI Collectives MPI_Reduce and MPI_Allreduce Operations on 64 GPUs (16 Nodes, 4 GPUs Per Node)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 246, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(a) GATHER - Small Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(b) GATHER - Large Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(c) ALLGATHER - Small Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(d) ALLGATHER - Large Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 5: Performance of MPI Collectives MPI_Gather and MPI_Allgather Operations on 64 GPUs (16 Nodes, 4 GPUs Per Node)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 246, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(a) BCAST - Small Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(b) BCAST - Large Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(c) ALLTOALL - Small Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(d) ALLTOALL - Large Message Sizes"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 6: Performance of MPI Collectives MPI_Bcast and MPI_Alltoall Operations on 64 GPUs (16 Nodes, 4 GPUs Per Node)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 314, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "uni-directional bandwidth performance at 32KB with 11 GB/s per-formance, OpenMPI + UCX at 1MB with 9.8 GB/s and Cray MPICH with 9.2 GB/s performance. In particular, we see lower bandwidth and bi-directional bandwidth for Cray MPICH in the message range between 8KB and 512 KB as demonstrated in Figures 3(c) and 3(d)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.3.3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Collective Operations -. We evaluate various collective operations including MPI_Reduce, MPI_Allreduce (Figure 4), MPI_Gather, MPI_Allgather (Figure 5), MPI_Bcast, and MPI_Alltoall (Figure 6) using the OSU-Micro-benchmarks suite. Various tests are included here specific to each MPI operation. The performance eval-uation demonstrates a comparison between four different communi-cation libraries (MVAPICH2-GDR, OpenMPI + UCX, Cray MPICH, and RCCL) on 64 AMD MI100 GPUs (16 nodes, 4 GPUs per node). In Figures 4, 5, and 6, one particular trend we noticed is that RCCL per-formance is typically not optimal for smaller message sizes between 4B-4KB, but performs well for large message allgather, and alltoall. For large message allreduce latency performance, MVAPICH2-GDR achieves 1.4 ms, OpenMPI + UCX achieves 160 ms, Cray MPICH demonstrates 1.8 ms, while RCCL performs at 1.5 ms. In Figure 6(a), we demonstrate small message broadcast performance for each of the libraries with MVAPICH2-GDR at 8.1 us, OpenMPI + UCX at 12.39 us, Cray MPICH at 12.06 us, and RCCL with 174.7 us at 4 Bytes."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 240, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "We demonstrate the importance of efficient Alltoall collective operation performance in Section 3.4 with the heFFTe application"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 246, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "which is heavily reliant on MPI_Alltoall or MPI_Alltoallv communi-cation. In figure 6(c), we evaluate the performance of small message GPU-aware Alltoall performance for MVAPICH2-GDR at 27.09 us, OpenMPI + UCX at 182.42 us, Cray MPICH at 40.21 us, and RCCL at 909.4 us at 4 Bytes."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "hanging": 8}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Overall, the performance discrepancies presented here for dif-ferent libraries can be a result of various components including, but not limited to: protocol changes, lack of tuning specific to a system or architecture, or underutilization of interconnect/link bandwidth. Through this evaluation, we highlight various areas that need to be optimized or accounted for in terms of communi-cation performance. In particular, the difference between the peak achievable performance for MPI libraries compared to the available link bandwidth presented by Infinity Fabric between GPUs and the Slingshot-10 network between nodes demonstrates the importance of link utilization and taking advantage of the vast performance made possible by these interconnects."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.4 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Application-Level Evaluation"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In this section, we evaluate the various MPI libraries at the ap-plication level. We use the heFFTE application detailed below to demonstrate GPU-aware MPI libraries' performance. In this case, the datatype required by heFFTe is not supported by RCCL and therefore RCCL is not included in the evaluation below. Due to compilation issues at the application layer with CrayMPICH and cmake, CrayMPICH is also emitted from this evaluation. We use"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PEARC '22, July 10-14, 2022, Boston, MA, USA"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Shafie Khorassani, et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 16, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 72, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(a) heFFTe - 16 GPUs (alltoall)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(b) heFFTe - 16 GPUs (alltoallv)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(c) heFFTe - 32 GPUs (alltoall)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(d) heFFTe - 32 GPUs (alltoallv)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 374, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 7: Performance of heFFTe Application using the rocfft backend for different problem sizes on 16 GPUs (4 nodes, 4 GPUs per node), and 32 GPUs (8 nodes, 4 GPUs per Node). Two different communication methods are shown including MPI_Alltoall [-a2a] (7(c)) and MPI_Alltoallv [-a2av] (7(d)) using various MPI libraries including MVAPICH2-GDR, and OpenMPI + UCX."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 50, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 156, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(a) rocHPCG - 8 GPUs (2 nodes, 4 ppn)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1356, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 68, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(b) rocHPCG - 16 GPUs (4 nodes, 4ppn)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1730, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 8: Performance of rocHPCG on 8 GPUs and 16 GPUs"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 2692, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "the rocHPCG application as well to compare the GPU-aware MPI libraries."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.4.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "heFFTE -. The heFFTe application is a highly efficient Fast Fourier transform (FFT) library for exascale systems. It uses GPU-aware MPI for communication and is provided as an open-source application. It provides the GPU kernel implementation with effi-cient scalability on large-scale clusters for 2-D and 3-D FFT libraries. Based on FFTMPI and SWFFT libraries, it presents so-called pencil-to-pencil methodology to compute 3-D FFT."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "We evaluate the performance of the heFFTe application as a measure of GFlops/s with different problem sizes. The application can be run with either an alltoall-based or alltoallv-based problem. When running heFFTe on GPUs using GPU-aware MPI libraries, we utilize the rocFFT backend provided for the heFFTe benchmarks with support for ROCm. We demonstrate the performance of heFFTe on GPUs in Figures 7(c) and 7(d) for alltoall with 65 GFlops/s and alltoallv with 187 GFlops/s using MVAPICH2-GDR for a problem size of 512\u22273, in contrast to 3.17 GFlops/s and 3.28 GFlops with OpenMPI + UCX for altoall and alltoallv, respectively, for the same problem size."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 216, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.4.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "rocHPCG -. rocHPCG [3] is a ROCm runtime benchmark based on the High-Performance Conjugate Gradients (HPCG) ap-plication for AMD GPUs. HPCG benchmark is used as a metric for the Top500 systems since it simulates the computational and data-access patterns of a variety of scientific applications, and com-munication patterns, including MPI point-to-point and collective operations and OpenMP supports. rocHPCG consists of different sub-operation metrics, including global dot product (DDOT), vector update (WAXPBY), sparse matrix-vector multiplication (SpMV),"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "multigrid preconditioner (MG), etc. We demonstrate the perfor-mance of each phase separately in the evaluation done in Figure 8 comparing MVAPICH2-GDR performance with OpenMPI + UCX."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "RELATED WORK"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The HPE Cray Slingshot Interconnect will be deployed on the up-coming exascale systems. De Sensi et. al [9] proposed early research investigating Slingshot for large-scale computing systems. They described Slingshot as the next-generation large-scale system and summarized the key features as the following: high-radix Ethernet switches, adaptive routing, congestion control, and QoS manage-ment. They evaluated the system performance using Slingshot with both individual and concurrent workloads to close the real HPC system usage. They found less congestion on Slingshot and the control algorithm is effective for most HPC and data center applica-tions. Also, a lower impact on performance from allocation policies was reported. Furthermore, Slingshot guarantees the bandwidth for jobs in different traffic classes."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 226, "right": 0, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The details of HPE Cray MPI are described in [14], including the latest implementation overview, HPE Cray MPI tuning and place-ment, GPU support, and its GPU-NIC asynchronous features. It also delves into the current support status with AMD and NVIDIA GPUs, including intra-node IPC and inter-node RDMA. Moreover, it intro-duced the GPU-NIC Async proposals, which decouples CPU-GPU control and data paths to reduce the CPU-GPU synchronization frequency and overheads."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Melesse Vergara et. al [13] elaborated on their experience of porting the current kernels of main applications to a novel plat-form with AMD GPUs and HPE/Cray programming environment. They ported GENASIS, Minisweep, and Sparkler to the HIP-based kernel and compared the performance. The experience of porting applications from CUDA-based to HIP-based kernel proved that"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 222, "right": 0, "firstLine": 198}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "High Performance MPI over the Slingshot Interconnect: Early Experiences"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "the porting procedure is easy, but there could be limitations, such as OpenMP support. Plus, additional tuning is required for fully utilizing the computing power on AMD GPUs. This work provided good examples for users to further port other kernel applications using HIP on AMD GPUs. Shafie Khorassani et. al [18] proposed an early research and designed a ROCm-aware MPI Library for the upcoming exascale systems, such as Frontier and El Capitan. They focused on Radeon Open Compute (ROCm) platform that adopts AMD GPUs. They utilized the ROCm features such as PeerDirect, ROCm-IPC, and large-BAR mapped memory to design a ROCm-aware MPI. An abstract communication layer with CUDA or ROCm backend allowed adaptability for the MPI runtime."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "CONCLUSION"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Next-generation exascale systems, and the first exascale and leading Supercomputer in the world, Frontier, are equipped with nodes con-nected by the HPE Cray Slingshot Interconnect. This interconnect technology is relatively new in the High-Performance Comput-ing realm and is seldom evaluated at the communication layer. In this work, we delved into a comprehensive evaluation and analy-sis of various state-of-the-art MPI libraries including MVAPICH2-GDR, OpenMPI+UCX, Cray MPICH, and RCCL on a system, Spock, equipped with the Slingshot-10 Interconnect to connect nodes over the network and with AMD MI100 GPUs. We demonstrate the performance of various point-to-point communication operations for latency and bandwidth and various collective operations on AMD Rome CPUs and GPU-aware communication on AMD MI100 GPUs. Due to the limitations of access to systems with the Sling-shot interconnect arising from its relatively new introduction, and limited accessibility of early access systems that emulate the ex-pected ecosystem of upcoming exascale systems, our evaluation is based on our early experiences with the system and with Slingshot-10 interconnect technology. In the future, we plan to extend this evaluation to cover additional applications with high demand for efficient communication performance, evaluate at a larger scale on a larger number of nodes based on system access, and ensure that state-of-the-art MPI and communication libraries provide the functionality, support, and efficiency that is to be expected with the growing demand and the rollout of Slingshot-11 networking."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACKNOWLEDGMENTS"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "We thank Dr. Sameer Shende (University of Oregon) for providing access to the Spock system. This research is supported in part by NSF grants #1818253, #1854828, #1931537, #2007991, and XRAC grant #NCR-130002. This research used resources of the Oak Ridge"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 250, "hanging": 8}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "PEARC '22, July 10-14, 2022, Boston, MA, USA"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 50, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Leadership Computing Facility at the Oak Ridge National Labora-tory, which is supported by the Office of Science of the U.S. Depart-ment of Energy under Contract No. DE-AC05-00OR22725."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 20, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "REFERENCES "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[1] 2021. Highly Efficient FFT for Exascale (HeFFTe) library. https://github.com/af-"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "ayala/heffte. Accessed: June 13, 2022."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[2] 2021. Radeon Open Compute (ROCm) Platform. https://rocmdocs.amd.com. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Accessed: June 13, 2022."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[3] 2021. rocHPCG. https://github.com/ROCmSoftwarePlatform/rocHPCG. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Ac-"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "cessed: June 13, 2022."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[4] 2021. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "ROCm "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Communication "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Collectives "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Library "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(RCCL). "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "https://github.com/ROCmSoftwarePlatform/rccl. Accessed: June 13, 2022."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[5] 2021. TOP 500 Supercomputer Sites. http://www.top500.org."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[6] 2021. Unified Communication X. http://www.openucx.org/. Accessed: June 13, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "2022."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[7] 2022. Frontier: ORNL's exascale supercomputer designed to deliver world-leading performance in 2021. https://www.olcf.ornl.gov/frontier/. Accessed: June 13, 2022."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 532, "right": 34, "hanging": 244}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[8] D. Bureddy, H. Wang, A. Venkatesh, S. Potluri, and D. K. Panda. 2012. OMB-GPU: A Micro-benchmark Suite for Evaluating MPI Libraries on GPU Clusters. In"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 526, "right": 0, "hanging": 238}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Proceedings of the 19th European Conference on Recent Advances in the Message Passing Interface (Vienna, Austria) (EuroMPI'12). 110-120."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 532, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[9] Daniele De Sensi, Salvatore Di Girolamo, Kim H. McMahon, Duncan Roweth, and Torsten Hoefler. 2020. An In-Depth Analysis of the Slingshot Interconnect."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 532, "right": 0, "hanging": 244}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "In SC20: International Confeorking, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Storage and Analysis. 1-14. [10] Edgar Gabriel, Graham E. Fangarra, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Jeffrey M. Squyres, Vishal Sahay, Prabhanjan Kambadur, Brian Barrett, Andrew "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Lumsdaine, Ralph H. Castain, David J. Daniel, Richard L. Graham, and Timothy S."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Woodall. 2004. Open MPI: Goals, Concept, and Design of a Next Generation MPI Implementation. In Proceedings, 11th European PVM/MPI Users' Group Meeting. Budapest, Hungary, 97-104."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 532, "right": 0, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[11] HPE. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "2022. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "HPE "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "SLINGSHOT "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "INTERCONNECT. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "https://www.hpe.com/us/en/compute/hpc/slingshot-interconnect.html."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Accessed: June 13, 2022."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 526, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[12] John Kim, Wiliam J. Dally, Steve Scott, and Dennis Abts. 2008. Technology-"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Driven, Highly-Scalable Dragonfly Topology. In 2008 International Symposium "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "on Computer Architecture. 77-88. "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[13] Veronica Melesse Vergara, Reube1. Early "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Experiences Evaluating the HPE/Cray Ecosystem for AMD GPUs. (7 2021)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "WORKSHOP. [14] "}, {"TYPE": "TabChar"}, {"TYPE": "TabChar"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "AY "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "MPI"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "-"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "SPOCK "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "https://www.olcf.ornl.gov/wp-content/uploads/2021/04/HPE-Cray-MPI-"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Update-nfr-presented.pdf. Accessed: June 13, 2022."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[15] OLCF. 2022. Oakridge National Laboratory: Leadership Computing Facility. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "https://www.olcf.ornl.gov. Accessed: June 13, 2022."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[16] OLCF. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "2022. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Spock "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Quick-Start "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Guide."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "https://docs.olcf.ornl.gov/systems/spock_quick_start_guide.html. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Accessed: June 13, 2022."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 530, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[17] Dhabaleswar Kumar Panda, Hari Subramoni, Ching-Hsiang Chu, and Moham-"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "madreza Bayatpour. 2020. The MVAPICH project: Transforming research into "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "high-performance MPI library for HPC community.omputational "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Science (2020), 101208. "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[18] Kawthar Shafie Khorassu, Chen-Chun "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Chen, Hari Subramoni, and Dhabaleswar K. Panda. 2021. Designing a ROCm-"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Aware MPI Library for AMD GPUs: Early Experiences. In High Performance "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Computing, Bradford L. Chamberlain, Ana-Lucia Varbanescu, Hatem Ltaief, and "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Piotr Luszczek (Eds.). Springer International Publishing, Cham, 118-136."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 224, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[19] Rajeev Thakur, Rolf Rabenseifner, and William Gropp. 2005. Optimization of Collective Communication Operations in MPICH. The InternHigh Performance Computing Applications 19, 1 (2005), 49-66."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 532, "right": 28, "hanging": 308}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "arXiv"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}
{"TYPE": "document", "VALUE": [{"TYPE": "body", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 288, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "SIJIE MAI, School of Electronics and Information Technology, Sun Yat-sen University, China "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "SONGLONG XING, School of Electronics and Information Technology, Sun Yat-sen University, China JIAXUAN HE, School of Electronics and Information Technology, Sun Yat-sen University, China YING ZENG, School of Electronics and Information Technology, Sun Yat-sen University, China HAIFENG HU, School of Electronics and Information Technology, Sun Yat-sen University, China"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 576, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal sequence analysis aims to draw inferences from visual, language and acoustic sequences. A majority of existing works focus on the aligned fusion of three modalities to explore inter-modal interactions, which is impractical in real-world scenarios. To overcome this issue, we seek to focus on analyzing unaligned sequences which is still relatively underexplored and also more challenging. We propose Multimodal Graph, whose novelty mainly lies in transforming the sequential learning problem into graph learning problem. The graph-based structure enables parallel computation in time dimension (as opposed to RNNs) and can efectively learn longer intra- and inter-modal temporal dependency in unaligned sequences. Firstly we propose multiple ways to construct the adjacency matrix for sequence to perform sequence to graph transformation. To learn intra-modal dynamics, a graph convolution network is employed for each modality based on the deined adjacency matrix. To learn inter-modal dynamics, given that the unimodal sequences are unaligned, the commonly-considered word-level fusion does not pertain. To this end, we innovatively devise graph pooling algorithms to automatically explore the associations between various time slices from diferent modalities and learn high-level graph representation hierarchically. Multimodal Graph outperforms state-of-the-art models on three datasets under the same experimental setting."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 174, "firstLine": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "CCS Concepts: \u00b7 Computing methodologies \u2192 Natural language generation; \u00b7 Theory of computation \u2192 Machine learning theory."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Additional Key Words and Phrases: Graph pooling, Multimodal Graph, Multimodal sequence analysis, Sentiment analysis"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "INTRODUCTION"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "With the development of the Internet and social platforms, there has been a large number of videos produced by users to express their views and posted online, which provides a source of multimodal data to analyze people's opinion in large quantities [3, 26, 28]. As videos are a typical form of multimodal information, people do not rely only on the spoken language, but they also use facial expressions and acoustic tones to convey information. In this paper, our downstream task is to use three modalities, i.e., language, visual and acoustic modalities, to draw inferences for the sentiment polarities of speakers [34]. These three modalities are complementary and actively interact with one another, providing more comprehensive information than one single modality."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Authors' addresses: Sijie Mai, School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; Songlong Xing, School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; Jiaxuan He, School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; Ying Zeng, School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; Haifeng Hu, School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 170, "right": 156, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proit or commercial advantage and that copies bear this notice and the full citation on the irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciic permission and/or a fee. Request permissions from permissions@acm.org."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 170, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u00a9 2022 Association for Computing Machinery."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 162, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1551-6857/2022/6-ART $15.00 "}, {"TYPE": "Break"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 7056, "hanging": 4}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Mai et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 82, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Hence, to maximally utilize the various information sources to capture the speaker's opinion with a multimodal architecture is a heated topic in multimodal sequence (language) analysis."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In the task of multimodal sequence analysis, two fundamental challenges exist, i.e., to learn the intra-modal dynamics of each modality, and the inter-modal counterpart for capturing cross-modal interactions [21, 59, 64]. The former relates to interactions that take place across time steps within one modality, while the latter is associated with interactions between diferent modalities. Previous researches mostly employ recurrent neural network (RNN) and its variants [7, 14, 19] to learn these two aspects [28, 54, 65], which, however, are slow in the inference process due to their recurrence in the time dimension. They are also prone to the problems of gradient vanishing and exploding as well as limited capacity of learning long-term dependency [4], which adds to the diiculty in learning of intra- and inter-modal dynamics. Particularly, it is of great signiicance to learn longer temporal dependency for unaligned sequences because they are often much longer [49]."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Exploring efective approaches to learn inter-modal dynamics has been one primary focus in the research of multimodal sequence analysis. To this end, a large portion of previous works fuse the three modalities at word level [28, 32, 54, 65, 66]. However, the interactions between various modalities are usually more complicated and last for longer than one word, i.e., cross-modal interactions may take place among words and the word-level fusion may break a complete interaction into multiple parts. Additionally, word-level fusion requires that the sequences are strictly aligned at word level. However, in real-world scenarios, such word-level alignment is time-consuming and computationally expensive [63]. Therefore, we claim that a fusion strategy should be able to dynamically and automatically associate various time steps from multiple modalities instead of considering fusion at each time step."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "To address the irst issue, we propose to utilize the high expressiveness of graph convolution networks (GCNs) to model unimodal sequential signals as an alternative to RNN. Recently, GCNs have attracted signiicant attention to model graph structured data, and yielded state-of-the-art performance on a broad variety of tasks [37, 46, 56]. GCN demonstrates high efectiveness in learning the relevance of nodes via the operation of convolution, and importantly, they dispense with the need for recurrence and can be computed in parallel, which greatly boosts eiciency in inferring time compared to RNN. Existing researches on GCN mainly utilize it for modeling graph-structured data. In contrast, in this work, we extend GCN to model sequential data and comparative analysis is conducted to show that GCN exhibits greater efectiveness compared to RNN [37, 46, 56] and temporal convolutional network (TCN) variants [1, 2]. With the operation of graph convolution, longer temporal dependency can be learnt by viewing each time step as a node and associating the relevant nodes even though they are far apart in time dimension. Note that graph neural networks are also utilized in previous works to conduct fusion[31, 57, 66], they have major diferences from our model (see Related Work section for more details)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 144, "firstLine": 206}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "However, one major obstacle to applying graph convolution to sequential learning is that sequences have no adjacency matrices as graph-structured data conventionally do. Therefore, the deinition and deduction of adjacency matrices is crucial. In this paper, we design multiple ways to achieve this goal, including non-parametric methods and learnable methods. In the non-parametric way, we mainly investigate the efectiveness of a proposed matrix, namely generalized diagonal matrix, which is extremely fast and almost free of computation. In the learnable way, we automatically learn the adjacency matrix via gradient descent (direct learning) and cross-node attention (indirect learning), which is more powerful and expressive. We present the comparative results of the proposed adjacency matrices in Section 5."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "For addressing the second problem, i.e., inter-modal dynamics learning, we elaborately design a graph pooling fusion network (GPFN), which learns to aggregate various nodes from diferent modalities by graph pooling without the need of time alignment information. Firstly we analyze the rationality of mean and max graph pooling approaches via mathematical deduction. However, mean and max graph pooling are still subject to some limitations in that they are not learnable and can only fuse neighboring nodes. Hence, to fuse the nodes in a"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 174, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "more expressive way, we further design a pooling strategy, termed link similarity pooling, which considers the association scores to the common neighbors of each two nodes. This learnable approach is based on the following two assumptions, i.e., (i) two nodes are closely related if their neighbors signiicantly overlap and thus can be fused; and (ii) provided the two nodes are neighbors, they are integrable with a high possibility. The link similarity pooling automatically updates the adjacency matrix and node embeddings for learning a high-level and reined graph representation."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 168, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "In conclusion, we propose a brand-new architecture named Multimodal Graph to address the sequential learning problem. The contributions of this paper can be summarized as:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022 We propose a novel graph-based architecture to model multimodal sequences, which innovatively trans-forms the sequential learning problem into a graph learning problem. Speciically, we design three unimodal GCNs to explore intra-modal dynamics, and a graph pooling fusion network to explore cross-modal inter-actions and fuse various nodes from diferent modalities."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 662, "right": 0, "hanging": 178}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022 We propose multiple approaches to deine adjacency matrices for sequential data, which can be extended to other sequential learning tasks. We compare the performance of diferent kinds of adjacency matrices empirically, and the visualization of the adjacency matrices is also provided to give insight on multimodal sequence analysis."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 662, "right": 174, "hanging": 178}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022 In GPFN, we investigate mean/max pooling and link similarity pooling to cluster the nodes hierarchically and thus learn high-level and reined graph representation, which dispense with the need of time alignment information for modality fusion. Specially, the proposed link similarity pooling considers the common neighbors of each two nodes (the topology information in the adjacency matrix) to better fuse the nodes from diferent modalities. To the best of our knowledge, we are the irst to leverage the power of graph pooling to conduct fusion in a hierarchical manner."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 662, "right": 168, "hanging": 178}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022 We show that the Multimodal Graph outperforms other methods on three widely-used datasets. Besides, the contrastive experiments against RNN and TCN variants demonstrate the efectiveness of GCN on modeling sequence, which indicate a novel approach in the research of sequence modeling tasks."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 662, "right": 144, "hanging": 178}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "RELATED WORK"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Multimodal Sequence Analysis"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal sequence analysis has attracted signiicant research interest in recent years [3, 65, 70]. Previous works focus on designing various fusion strategies to explore inter-modal dynamics. One of the simplest ways to explore inter-modal dynamics is to concatenate features at input feature level, which shows improvement over single modality [44, 45, 55]. In contrast, a large number of publications irstly infer decision according to each modality and combine the decisions from all modalities using some voting mechanisms [23, 39, 67]. However, as elaborated by Zadeh et al. [64], these two types of methods cannot efectively model inter-modal dynamics."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 170, "firstLine": 8}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Consequently, more advanced fusion strategies are proposed in the past few years. Speciically, performing tensor-based fusion has received much attention [20, 27, 29, 33]. Tensor Fusion Network (TFN) [64] and Low-rank Modality Fusion (LMF) [29] adopt outer product to learn joint representation of three modalities. More recently, Mai et al. [30] propose a 'Divide, Conquer and Combine' strategy to conduct local and global fusions. Interpretable multimodal fusion has also received high attention recently. For example, Multimodal Routing [50] applies routing mechanism in capsule network to discover which interactions are importance for predicting each emotion, and Li et al. [26] address interpretable issue using quantum theory. Furthermore, some translation methods such as Multimodal Transformer (MulT) [49] aim at learning a joint representation by translating source modality into target modality. For graph-based methods, Graph-MFN [66] and Graph Fusion Network (GFN) [31] apply graph neural network to fuse features. Although graph neural network is used in these methods, the proposed model signiicantly difers from them. Firstly, they do not fuse features across the"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Mai et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 80, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "time dimension and merely regards each modality as one node, whereas we view each time step in each modality as one node and perform graph convolution over the node embeddings. Secondly, we employ graph convolution and graph pooling for fusion as well as propose multiple adjacency matrices for sequence learning, which are not involved in Graph-MFN and GFN. More recently, Modal-Temporal Attention Graph (MTAG)[57] applies graph convolutional model to analyze multimodal sequential data, which uses dynamic pruning and read-out technique to explore cross-modal interactions and obtain multimodal embedding. However, MTAG does not comprehensively deine adjacency matrix for sequences, and simply uses average operation to perform graph readout. In comparison, Multimodal Graph proposes novel graph pooling algorithm to fuse nodes and learn high-level graph representation hierarchically, and provides multiple ways to deine the adjacency matrix for sequences."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 168, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "To avoid sarcasm and ambiguity, many methods learn cross-modal interactions at word level such that various modalities are aligned at time dimension [6, 28, 32, 34]. For instance, Memory Fusion Network (MFN) [65] uses systems of LSTM to learn intra-modal dynamics, and it implements delta-memory attention and multi-view gated memory network to fuse memories of LSTMs across time. In addition, Multi-Fusion Residual Memory (MFRM) [32] applies multi-stage fusion to fuse the three modalities, and it designs a residual memory network to capture the long-term dependency. However, in unaligned multimodal sequence, word-level fusion cannot be performed."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A clear distinction of the majority of these previous methods between our Multimodal Graph is that we do not apply any RNN or TCN variants to learn intra-modal and inter-modal dynamics. Instead, we investigate the efectiveness of graph convolution and graph pooling on modeling multimodal sequence. Our Multimodal Graph is very elegant and efective, which can efectively learn longer temporal dependency by directly associating the distant related nodes and allow parallel computing at time dimension."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Graph Neural Networks (GNNs)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A graph can be denoted as G = (N, E), where N = {n1,n2, ...,nT } is a set of nodes, and E \u2286 N \u00d7 N refers to the set of observed edges. The set of node embeddings is denoted as N \u2208 RT \u00d7dwhere d refers to the dimensionality of each node embedding and T is the number of nodes. The edges can also be described using the adjacency matrix A \u2208 RT \u00d7T. Each element of A is non-negative and Ai,j = 0 means that nodes i and j are not connected. A wide variety of GNNs have been proposed in the last few years [11, 37, 46, 56]. We mainly focus on GCNs [25] in this paper. GCNs have become increasingly popular recently due to its applicability to graph structured data and yielded state-of-the-art performance on a variety of learning tasks, such as node classiication [16], link prediction [36, 68], image retagging [48], group activity recognition [47], and graph classiication [69]. GCN is basically a convolutional operation on the nodes that are directly connected. The kthiteration of the general GCN can be described as: "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(ai)k\u22121= AGGREGATE((Nj)k\u22121; j \u2208 \u03b7(i)) "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(1)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(Ni)k= COMBINE((Ni)k\u22121, (ai)k\u22121) "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(2)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 3188, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where (Ni)kis the embedding for node i at iteration k, \u03b7(i) represents the set of 1-hop neighbors of node i, the AGGREGATE function aggregates information from 1-hop neighbors of node i and output the aggregation representation (ai)k\u22121, and the COMBINE function combines the information of node i and its aggregation information (ai)k\u22121to update the embedding of node i."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "hanging": 8}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Among all GCNs, those trying to learn or recover adjacency matrices are related to our method. Franceschi et al. [10] use bi-level program to irst sample adjacency matrix and then learn the parameters for the graph by minimizing inner and outer objectives. In contrast, in the direct learning way, we directly parameterize the adjacency matrix as a learnable matrix and jointly learn the adjacency matrix and graph parameters via gradient descent. Also, we provide multiple ways to deine the adjacency matrix. As for graph pooling, the DifPool [60] learns a diferentiable soft cluster strategy for nodes using node embedding, mapping nodes to a set of clusters."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Fig. 1. The Schematic Diagram of Multimodal Graph. Multimodal Graph consists of three Unimodal Graphs and a Graph Pooling Fusion Network (GPFN). Adj denotes adjacency matrix."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In contrast, we utilize the adjacency matrix to learn the cluster assignment matrix, which considers the neighbor similarity of nodes. Compared to using node embedding to learn cluster assignment matrix, using adjacency matrix is more intuitive and simple. StructPool[62] uses conditional random ields to capture the high-order structural relationships among diferent nodes to learn a node cluster assignment matrix based on the node features, where the adjacency matrix is used to ind the topological information of the graph. In contrast, we directly utilize the link (neighbor) information in the adjacency matrix to learn the cluster assignment matrix."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 168, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "There also exist several works that try to apply GNNs to multimodal learning tasks. Gao et al. [12] apply fully-connected visual, semantic and numeric graphs to represent an image and conduct message passing between the graphs to utilize the contexts in various modalities. Nevertheless, the graph here is fully-connected and only represents an image. Misras et al. [38] connect every image with its corresponding tags and the image's K-nearest neighbors to capture visual-semantic relationship. Similarly, Wang et al. [53] learn a sparse multigraph construction from multimodal images. Nevertheless, these graphs are not targeted for sequential learning. Ji et al. [22] propose a cross-modal hypergraph to capture the noisy correlation among heterogeneous modalities, where the relevance of the nodes is calculated by Euclidean distance. In contrast, we propose multiple approaches to explore the relevance between nodes in the graph, and introduce graph pooling to learn reined representations for multimodal graph. Generally speaking, none of these approaches are similar to our work in terms of the main contributions: 1) we propose multiple ways to deine adjacency matrix for sequences and transform the sequential learning problem into a graph learning problem; 2) we propose new graph pooling algorithms to fuse the unimodal nodes and thus learn high-level and reined graph representations; 3) we conduct extensive experiments to verify that graph-based structure can outperform popular sequential models, which indicates a new methodology in the research of sequential learning."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 170, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "MODEL ARCHITECTURE"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In this section, we describe Multimodal Graph in detail, with its diagram illustrated in Fig. 1 and the procedure is summarized in Algorithm 1. Multimodal Graph is hierarchically structured to cater to two stages, i.e., intra-modal and inter-modal dynamics learning. In the irst stage, we propose to employ a graph convolution network (GCN)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 162, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "6"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Mai et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 82, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Algorithm 1: Procedure for Multimodal Graph "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Input: Raw sequences of unimodality Na\u2208 RTa\u00d7da, Nv\u2208 RTv \u00d7dv, and Nl\u2208 RTl \u00d7dl. Output: The sentiment prediction."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 2736, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In Unimodal Graphs: "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "For each modality do: "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Compute adjacency matrix by Eq. (3)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 5904, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Perform graph convolution by Eqs. (4)-(7)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 668, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "End "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "In GPFN: "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Perform node sorting to obtain multimodal sequence as in Section 3.3.1."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 3600, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Deine adjacency matrix for multimodal sequence by Eq. (3)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 442, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Perform graph convolution and graph pooling as in Fig. 2."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 442, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Obtain graph representation as in Section 3.3.4."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 442, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Perform prediction as in Section 3.3.4."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 442, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "for each modality, termed as Unimodal Graph. In the second, a graph pooling fusion network (GPFN) is devised for capturing cross-modal interactions. As an early attempt to employ GCN for sequential learning, the architecture of Multimodal Graph is free of RNNs which are commonly used for sequential learning but subject to a number of limitations such as slowing inferring speed. Extensive discussion on applying this graph-oriented approach to sequential data is also provided in this section, including the deinition of adjacency matrices. Moreover, with the proposed graph pooling algorithms in GPFN, our model can fuse the unaligned unimodal sequences and learn high-level and reined multimodal representation."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 170, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Notations and Task Definition"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 1. Table of Notations"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1388, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Notations"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Descriptions"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Ga= (Na, Ea) Na "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Nm "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "T "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "A"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 96, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "f "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "\u03bb "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "\u03f5 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "R "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "s"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 432, "right": 432, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Graph for modality a with node Naand edge Ea Sequence (node embedding) for modality a "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Node set for multimodal sequence "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "The number of nodes (time steps) in a sequence Adjacency matrix "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Nonlinear activation function "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Attenuation factor for Generalized Diagonal Matrix A small negative scalar to prevent division by zero Matrix transposition operation "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Pooling size for mean/max pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 2. Table of Abbreviation"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 498, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Abbreviation"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Full Name"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "GPFN "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "GCN "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Adj "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "LSP "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "GDM "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "TCN "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "KNN "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "RNN "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "GRU "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "LSTM"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Graph Pooling Fusion Network "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Graph Convolution Network "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Adjacency Matrix "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Link Similarity Pooling "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Generalized Diagonal Matrix "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Temporal Convolution Network K-Nearest Neighbor "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Recurrent Neural Network "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Gated Recurrent Units "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Long Short-Term Memory Network"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Our downstream task is multimodal sentiment analysis and emotion recognition. The input to the model is an utterance [40], which is a segment of a video bounded by pauses and breaths. Each utterance has three modalities, i.e., acoustic (a), visual (v), and language (l). The sequences of acoustic, visual, and language modalities are denoted as Na\u2208 RTa\u00d7da, Nv\u2208 RTv \u00d7dv, and Nl\u2208 RTl \u00d7dl, respectively. We aim to predict the sentiment score or emotion of the utterance using the unimodal sequences. We summarize the frequently-used symbols and abbreviations in Table 1 and 2 for convenience."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Unimodal Graphs"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "To address the irst challenge, i.e., learn intra-modal interactions, we utilize three unimodal GCNs on unimodal sequences, each for one modality. Intra-modal interactions are essential in multimodal language analysis which involves the modality-speciic information. The majority of prior works leverage RNNs to explore intra-modal"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "dynamics [28, 32, 54, 65], which are prone to some limitations as stated in the Introduction section. Moreover, TCNs and RNNs use ixed patterns to model sequence, which are not lexible as they cannot automatically link related time steps. In contrast, we propose to use GCNs to learn unimodal high-level representations for each modality. With a suitable adjacency matrix, GCNs enable parallel computation in the time dimension and is able to learn long-term temporal contextual information by identifying and connecting related time steps."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 144, "firstLine": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "We deine the acoustic, visual, and language graphs as Ga= (Na, Ea), Gv= (Nv, Ev), and Gl= (Nl, El), "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "2, ...,na Ta} is a set of acoustic nodes, Ea \u2286respectively. Taking the acoustic graph as an example, Na= {na 1,na "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Na\u00d7 Narefers to the set of edges that directly connects the acoustic nodes. The acoustic node embedding is denoted as Na\u2208 RTa\u00d7da, which is the input acoustic sequence."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "However, unlike graph-structured data, a unimodal sequence does not have an adjacency matrix that determines the graph topology. Hence, one major problem is to deine the adjacency matrix in the unimodal sequence such that it efectively relects the connection between nodes (time steps). Intuitively, two nodes are assumed to be connected if they are close in terms of the feature embedding. Therefore, we can measure the similarity between the embeddings of each two nodes to determine whether they are neighbors. Here we use a simple cross-node attention mechanism to determine the correlation between nodes and thus deine the adjacency matrix for unimodal sequences. The equations are shown below (taking acoustic modality as an example):"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 170, "firstLine": 200}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u02c6Aa= f [f (QWa 1)f ((PW a 2)R)], "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Aa i,j=\ufffdv \u2208N\u02c6Aa "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "\u02c6Aa i,j"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2364, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "i,v+ \u03f5 "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(3)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6288, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where Q = f (NaWa q) \u2208 RTa\u00d7d, P = f (N aW a p) \u2208 RTa\u00d7d, and W a 1\u2208 Rd\u00d7d,W a 2\u2208 Rd\u00d7d,W a q\u2208 Rda\u00d7d,W a p\u2208 Rda\u00d7d"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "are learnable matrices. f is the nonlinear activation function to increase the nonlinear expressive power of"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "the model and R denotes the matrix transpose operation. In the learned adjacency matrix, the values can be interpreted as the intensity of interactions between nodes. Therefore, the negative links relect little or no interaction between the corresponding two nodes. We apply ReLU as our activation function such that the negative links between nodes can be efectively iltered out. The equations are the same for each modality except that the node embedding to learn the adjacency matrix is diferent. This approach to constructing an adjacency matrix for a temporal sequence is learnable with parameters, and meanwhile it is instance-speciic as it considers the various relatedness among nodes for diferent utterances, as opposed to directly setting all the matrix elements as learnable parameters (we will discuss it in Section 4). Hence, we term this approach indirect learning. We claim that this instance-speciic and learnable approach can capture more relatedness information on the nodes and generate more favourable performance, as shown in Section 5.5. From this deinition of adjacency matrix for sequential data, it can be seen that even if the two nodes are distant apart in the time dimension, they can still be directly connected if they are considered related. Therefore, compared to RNN variants, GCN can efectively learn long-term temporal dependency with fewer layers, which makes it a suitable alternative to model long temporal sequences. Compared to TCN variants that use ixed convolution pattern to model sequences, the proposed method is more lexible to automatically identify and recognize related time steps. Although the indirect learning method has some similarity to Transformer [51] in avoiding recurrence and learning long-term dependency, this approach is diferent from it in several aspects. Firstly, Transformer uses sof tmax as activation function, which means that each time step is associated with all time steps. In contrast, our method is better-targeted in that it can ilter out the time steps that have no direct connection, and automatically detect the one-hop neighbors for each node. Moreover, after inding the neighbors for each time step, we use a GCN such as Graph Isomorphism Network (GIN) [56] to aggregate the information of the neighbors and explore the inter-dependency between time steps, and this operation is quiet diferent from the feed forward network of Transformer which only operates at the feature dimension and cannot explore the connections between time steps."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 168, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Mai et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 82, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Note that in common graph deinition, the elements in the adjacency matrix are often binary and restricted to either 0 or 1, which denotes no/one direct connection, respectively. However, we dispense with this restriction and formulate the elements to be continuous, with a larger value indicating a closer relation between two nodes, and vice versa. This can be interpreted as multiplying an attention mask matrix to the conventional adjacency matrix. We justify the use of such continuous soft weights in adjacency matrix in Appendix."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "After obtaining the adjacency matrix, the deinition of COMBINE and AGGREGATE functions in GCN could have many choices. It is worth mentioning that the unimodal graphs are independent of the concrete GCN model. In other words, we can integrate any GCN model into our unimodal graph. In practice, we compare the performance of Graph Isomorphism Network (GIN) [56], Graph Attention Network (GAT) [52], GraphSAGE [16] and DifPool [60] in our experiment. Speciically, we use GraphSAGE with mean pooling [16] as the default GCN in this paper, and the equations for the kthiteration of GraphSAGE is shown below:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "firstLine": 200}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(\u02c6Na)k= f (D\u22121(Aa+ I)(Na i)k\u22121W k a)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1376, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(4)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 94, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(Na i)k ="}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 72, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(\u02c6Na i)k"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(5)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 94, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "||(\u02c6Na i)k ||2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where f is the non-linear activation function for which we use ReLU in our experiment, (Na i)k is the hidden"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "representation for node i of the acoustic modality at iteration k, andWk ais the parameter matrix. D is the diagonal"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "degree matrix of Aawhere Dii =\ufffdmean pooling, and the identity matrix I is added to the adjacency matrix Aato perform self-loop operation. jAa ijand Dij = 0 (i \ufffd j). The diagonal degree matrix D is added to perform"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Normalization is done in Eq. 5. To obtain the inal unimodal representation, the hidden representations (Na i)k"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "for each layer is concatenated and sent to the fully-connected layers:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 94, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Na i\u2190 \u2295(N a i)k, k \u2208 [1, 2, ..,r]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1618, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(6)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 94, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Na i= f (N a iW a o+ ba o)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 1996, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(7)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 94, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where Na iis the inal representation of node i for acoustic modality, \u2295 denotes concatenation, r is the number of"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "layers, and Wa oand ba oare the weight matrix and bias for the fully connected layers, respectively. Note that we"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "use the same convolution structure for each modality."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "With graph convolution, intra-modal interactions can be explored efectively. Unlike the commonly used RNN variants which are subject to a number of issues such as gradient vanishing and explosion, forgetting problem and slow inferring speed, GCN abandons the recurrence and can operate in parallel, which is more eicient in terms of time complexity and can learn longer temporal dependency. More importantly, it detects the immediate (one-hop) neighbors for each time step and ilters out the unrelated pairs, which is better-targeted compared to Transformer. Extensive experiments are conducted in Section 5.4.1 to show the superior performance of GCN in modeling sequential data."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 174, "firstLine": 206}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Graph Pooling Fusion Network (GPFN)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "After exploring intra-modal interactions, the second challenge is to model inter-modal dynamics and fuse the cross-modal nodes. Considering that we focus on unaligned sequences, the common word-level fusion cannot be achieved. This means that our fusion network should learn the interactions among various nodes from multiple modalities, rather than fuse the features from three modalities at each time step. To this end, we devise GPFN to fuse the unaligned sequences. GPFN learns to aggregate the multimodal nodes to learn high-level and reined graph representations hierarchically. Speciically, in GPFN, we introduce max/mean graph pooling and analyze their rationality, which are suitable for pooling the unimodal nodes. Additionally, we propose link similarity pooling to learn a cluster assignment matrix using the link (topology) information of adjacency matrix, which can fuse the cross-modal nodes. Since features from diferent modalities are highly heterogeneous,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 152, "hanging": 8}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "9"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Fig. 2. The Detailed Structure of GPFN. s denotes pooling size. The arrow in the figure indicates the direction in which the layers are stacked, from botom to top."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 144, "firstLine": 8}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "the interactions between the cross-modal nodes are much more complex than those of the nodes from a single modality. By applying link similarity pooling, the model can automatically learn the complex interactions between the heterogeneous modalities by associating the related nodes between these modalities."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "As shown in Fig. 2, GPFN mainly consists of node sorting, adjacency matrix deinition, graph convolution, mean/max graph pooling, link similarity pooling, graph representation deinition and prediction inference. To retain consistence, the graph convolution framework and the adjacency matrix deinition method are the same as those in Unimodal Graphs. In the following subsections, we will introduce node sorting, max/mean graph pooling, link similarity pooling, graph representation deinition and prediction inference respectively."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.3.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Node Sorting. Firstly, we need to arrange the nodes to determine the order of the nodes from the three Unimodal Graphs and obtain the multimodal sequence. An intuition here is to sort nodes from three modalities according to time dimension such that the nodes from neighboring time steps are closer to each other. However, this requires that the time dimensions of diferent modalities are explicitly aligned. Hence, we simply concatenate the nodes at the time dimension one modality after one modality, and let the model automatically learn to aggregate these nodes. The node set Nmcan be described as Nm= {nl 1,nl 2, ...,nl Tl,na 1,na 2, ...,na Ta,nv 1,nv 2, ...,nv Tv} = {nm 1,nm 2, ...,nm Tl +Ta+Tv}. For conciseness, we denote the multimodal sequence as N \u2208 RT \u00d7d in the rest of the paper, and the adjacency matrix for multimodal sequence is denoted as A \u2208 RT \u00d7T, where T is equal to Tl +Ta +Tv."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.3.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Mean/Max Graph Pooling. The initial adjacency matrix is computed in the same way as the Unimodal Graphs. With graph pooling, the related nodes are fused so that the interactions can be explored and the new adjacency matrix can be obtained. In the GPFN, we provide two pooling approaches. The irst kind is the simple max pooling and mean pooling. For the adjacency matrix, 2D mean/max pooling is applied, while for the node"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "10"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Mai et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 94, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "embeddings, 1D mean/max pooling is applied. The equations for mean pooling are given as follows:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Nk i="}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 44, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffds\u22121 \u0434=0N k\u22121 s \u00b7i\u2212\u0434"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 36, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(8)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 74, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "s"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 470, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Ak i,j="}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 26, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffds\u22121"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffds\u22121 m=0Ak\u22121 s \u00b7i\u2212\u0434,s \u00b7j\u2212m"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 40, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(9)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 74, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "s2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 444, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "and for max pooling: "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Nk i= max({N k\u22121 s \u00b7i\u2212\u0434| 0 \u2264 \u0434 \u2264 s \u2212 1}) "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(10)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Ak i,j= max({Ak\u22121 s \u00b7i\u2212\u0434,s \u00b7j\u2212m| 0 \u2264 \u0434 \u2264 s \u2212 1, 0 \u2264 m \u2264 s \u2212 1}) "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(11)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2400, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where s is the pooling size, Nk iis the updated node embedding for node i at iteration k, and \u00b7 denotes scalar multiplication. Note that in Eq. 10, the max pooling operation is element-wise."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "hanging": 8}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "The mean/max pooling is meaningful because the nodes in multimodal sequence are concatenated according to the time dimension of each unimodal sequence, and thus the neighboring nodes are closely related in time dimension and considered to be fusible (but we need to carefully determine the pooling size s to avoid fusing the nodes from diferent modalities). Moreover, we have the following observation: "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Observation 1: If nodes x and y are 1-hop neighbors (directly connected), then they are 1-hop or 0-hop neighbors after mean/max graph pooling, where 0-hop neighbors mean that x and y are merged into the same node after graph pooling."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Proof: See Appendix."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 374, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "The above property of mean/max pooling suggests that they are reasonable approaches for graph pooling, and indicates a principle for us to manually design graph pooling algorithms: once neighbors, always neighbors."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Although the mean/max graph pooling may be efective in fusing nodes from the same modality given that we can utilize the time information in the unimodal sequence, it is not a desirable method for fusing nodes from diferent modalities in unaligned multimodal sequence. Therefore, we need a learnable method that can efectively cluster the heterogeneous cross-modal nodes. To this end, we propose link similarity pooling in the following section."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 174, "firstLine": 208}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.3.3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Link Similarity Pooling. Apart from the mean/max graph pooling, we devise a learnable graph pooling method, named link similarity pooling, to leverage the link information (topology information) in the adjacency matrix to learn the node cluster assignment matrix. Diferent from other graph pooling methods such as DifPool [60] that mainly utilize the node embedding to learn a cluster assignment matrix [60, 62], link similarity pooling uses the neighbor (topology) information in the adjacency matrix to learn a cluster assignment matrix, which is more intuitive and interpretable. We deine the link similarity pooling in following equations:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Z\u2032= AAR,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 78, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Z = f [(Z\u2032+ A)Wz]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 98, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(12)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 74, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where R denotes matrix transposition, Wz \u2208 RT \u00d7T \u2032is the transfer parametric matrix, and Z \u2208 RT \u00d7T \u2032is the inal node cluster assignment matrix that maps the T nodes into T\u2032nodes (T\u2032is sequence length after pooling). The irst equation measures the similarity score of two nodes by calculating the inner product of their respective association intensity to their common neighbors. In this way, a greater extent to which a pair of nodes have similar distribution of association intensity on their shared neighbors leads to a larger score. It is also noteworthy that this score is weighted because the elements in the adjacency matrix are soft (continuous) and not restricted to be binary. For example, if nodes x and y have more common neighbors and their linked values with the common neighbor are larger, then Z\u2032of common neighbors between x and y. The second equation adds the original adjacency matrix to the link x,yis larger. In Fact, we have Z \u2032x,y= \ufffdc \u2208CNAx,c\u00b7 Ay,cwhere CN denotes the set"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 168, "hanging": 8}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "11"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "similarity information, which means that Zx,y is larger if x and y are 1-hop neighbors. This is reasonable because if two nodes are neighbors, then they are considered to be similar and thereby can be fused with a high possibility."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "After obtaining the node cluster assignment matrix Z, we use it to learn the updated adjacency matrix and node embedding. The equations are presented as follows:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S = f (NWs) "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(13)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4156, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Nupdate = ZRS, Aupdate = ZRAZ "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(14) where S is the transformed node embedding, Ws \u2208 Rd\u00d7dis a learnable parameter matrix, Nupdate \u2208 RT \u2032\u00d7dand Aupdate \u2208 RT \u2032\u00d7T \u2032denote the updated node embedding and adjacency matrix, respectively."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The reason why we do not use the node embedding is that the adjacency matrix is originally derived from the node embedding, and therefore adjacency matrix contains part of the information in node embedding as well as the topology information. Therefore, we assume that using the adjacency matrix is suicient to learn the node cluster matrix."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.3.4 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Graph Representation Definition and Prediction Inference. After the graph convolution and graph pooling operation of GPFN, we average the node embeddings as the representation of GPFN. Then we concatenate the representation of GPFN with the averaged node embedding from three Unimodal Graphs respectively to obtain the inal graph representation. Several fully-connected layers are applied on the graph representation to infer the inal sentiment decision."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "DISCUSSION ON THE ADJACENCY MATRIX"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Previously we deine an indirect learning method to construct the adjacency matrix for sequential data, which is instance-speciic and learnable. In this section, we aim to explore other methods to construct the adjacency matrix. We present three other ways to deine the adjacency matrix, namely generalized diagonal matrix, KNN-based adjacency matrix, and a direct learning method."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 164, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Generalized Diagonal Matrix"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Intuitively, the neighboring time slices are more related to each other. Therefore, we deine a generalized diagonal matrix (GDM) to relect this point:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 36, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bb"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bb2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bbn\u22121"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 70, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(15)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 74, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bb"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bb"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bbn\u22122"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bbn\u22121"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 56, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bbn\u22121"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bb"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bb"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 36, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 70, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "..."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bb2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u03bb"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where \u03bb is the attenuation factor which is set to 2 in our experiment, and n is the truncation factor which is set to 10. The value on the diagonal of the GDM is 1, and each element is decayed by a factor of1 \u03bbcentered on the diagonal, which means the correlation between them is reduced with the increase of distance. When the distance to the diagonal element is larger than n, the value becomes zero, which means that the distant nodes no longer have direct connection. Nevertheless, we can stack many layers so that each node can still have the overall view of the input sequence. Obviously, for a sequence of length T, we need to stack ceil((T \u2212 1)/(n \u2212 1)) layers such that each node can incorporate information from all the nodes, where ceil means rounding up to an integer."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 166, "hanging": 8}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "An advantage of GDM lies in that it eschews the complex computation for inding an adjacency matrix. Actually, GDM functions like the kernels in TCN [1, 2, 41]. But unlike TCN kernels, it is predeined instead of being obtained through learning (we leave the learning part to the parameters of GCN). The main diferences between GCN and TCN will be discussed in Appendix. In addition, we also implement a fully-connected adjacency"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 144, "firstLine": 208}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "12"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Mai et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 94, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "matrix whose values are all ones to make a comparison. Compared to the indirect learning method, this GDM method is intuitive and its the empirical pattern of sequence modeling, but it is neither instance-speciic nor learnable. GDM serves as a reasonable baseline in the exploration of adjacency matrix for sequential data."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 170, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "KNN-based method"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Another simple but efective non-parametric approach to inding an adjacency matrix in a sequence is to apply K-nearest neighbor (KNN) algorithm to deine the 1-hop neighbors of each node, which has also been evaluated in [10]. KNN is based on Euclidean distance, and we select the nodes with shortest distance as the 1-hop neighbors for each node. The equations can be described as below:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 168, "hanging": 8}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "1 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "dij = "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Eur(Nj; Ni) + \u03f5"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 3700, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u02c6Ai,j = ReLU(dij \u2212 \u03b1 \u00d7\ufffdjdij"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 26, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "T"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 212, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": ")"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 26, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(16)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 74, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(17)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 74, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Ai,j =\ufffdv \u2208N\u02c6Ai,v "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "\u02c6Ai,j "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(18)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 3870, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where Eur denotes the Euclidean distance, dij denotes the 'similarity' of node j to node i, \u03f5 denotes a positive"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "scalar to prevent division by zero, and \u03b1 is a scalar that controls to what extent the weak links can be iltered out. Eq. 17 ilters out the links that have no strong connection and Eq. 18 denotes simple normalization. Note that by applying Eq. 17, we do not select an exact number of k neighbors for each node, but we allow a variable number of neighbors, as long as the links between the node and its neighbors meet the threshold. A disadvantage of KNN-based method is that it has to compute the Euclidean distance of each two nodes for each instance, making it more time-consuming compared to GDM. By deinition, KNN-based adjacency matrix is instance-speciic, but it is not learnable, serving as a reasonable comparative method to the indirect learning method."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Direct Learning"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Another intuitive way to construct an adjacency matrix is to learn the matrix directly in the training process. In this method, the adjacency matrix is parameterized as a learnable matrix\u02c6A \u2208 RT \u00d7T, and a ReLU function is applied to activate the learned matrix: A = ReLU(\u02c6A) such that the linked values between nodes are non-negative."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "To obtain a more representative adjacency matrix, we add a regularization term, as shown below:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 194, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2113=\ufffd(\ufffd\u02c6Ai,j)2+\ufffd(\ufffdReLU(\u02c6Ai,j)\u2212\u03b3 )2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 2158, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(19)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 74, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "i \u2208N"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 48, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "j \u2208N"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 54, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "i \u2208N"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 40, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "j \u2208N"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 62, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where the irst term forces the sum of linked values of each node to equal zero, which can prevent the learned matrix from becoming a fully-connected matrix; the second term restricts the sum of the positive linked values of each node to approximate a given positive scalar \u03b3 so as to prevent it from degenerating into an all-zero matrix. \u2113 is optimized via gradient descent. Note that the learnt adjacency matrix is shared across instances (instance-independent), which may not be expressive enough compared to the instance-speciic indirect learning method but is computationally eicient."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "hanging": 8}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "EXPERIMENTS"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph is evaluated on three popular datasets for multimodal learning. In this section, we focus on the following questions: 1) Does Multimodal Graph perform favorably to TCN and RNN variants? 2) Does Multimodal Graph achieve state-of-the-art performance on multimodal sentiment analysis and emotion recognition? 3) What kind of GNNs performs best in our Multimodal Graph? 4) What kind of adjacent matrix performs best? 5) What are the attributes of adjacent matrices?"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "\u2022"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "13"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Datasets "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "5.1.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "CMU-MOSI. CMU-MOSI [67] is a widely-used dataset for multimodal sentiment analysis. It contains 93 videos in total, and each video is divided into 62 utterances at most. The intensity of sentiment ranges within [-3,3], where -3 indicates the strongest negative sentiment, and +3 the strongest positive. We evaluate model's performance using various metrics, including 7-class accuracy (i.e., Acc7: sentiment score classiication), binary accuracy (i.e., Acc2: positive or negative sentiments), F1 score, mean absolute error (MAE) of the score, and the correlation of the model's prediction with humans (Corr). To be consistent with prior works, we use 1,284 utterances for training, 229 for validation, and 686 for testing."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.1.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "CMU-MOSEI. CMU-MOSEI [66] is the largest multimodal language analysis dataset that contains a total number of 2928 videos. The dataset has been segmented at the utterance level, and each utterance has been scored on two levels: sentiment ranging between [-3, 3], and emotion with six diferent values. We use the sentiment label in our task. In our experiment, the evaluated metrics for CMU-MOSEI are the same as those for CMU-MOSI dataset. We use 16,265 utterances as training set, 1,869 utterances as validation set, and 4,643 utterances as testing set."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.1.3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "IEMOCAP. IEMOCAP [5] is a multimodal emotion recognition dataset that contains a total number of 151 videos from 10 speakers. The videos are segmented into about 10K utterances. IEMOCAP has the following labels: anger, happiness, sadness, neutral, excitement, frustration, fear, surprise and other. We take the irst four emotions to compare with our baselines. We follow previous works[49, 65] to report the classiication accuracy and the F1 score of each emotion."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 170, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Baselines"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The baselines for comparison include Early Fusion LSTM (EF-LSTM), Late Fusion LSTM (LF-LSTM), Ten-sor Fusion Network (TFN) [64], Memory Fusion Network (MFN) [65], Multi-Fusion Residual Memory Network (MFRM) [32], Multimodal Transformer (MulT) [49], and Modal-Temporal Attention Graph (MTAG) [57]. Notably, for EF-LSTM, MFN, and MFRM, since they adopt word-level fusion, connectionist tempo-ral classiication (CTC) [15] is performed to process the unaligned sequences to obtain approximately aligned sequences. The models are trained to optimize the CTC alignment objective and the prediction objective simulta-neously. The detailed introduction of baselines are shown in Appendix for lack of space."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Experimental details"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Hyperparameter Setting: We develop our model on Pytorch with RTX2080Ti as GPU. We apply Mean Absolute Error (MAE) as loss function with Adam [24] as optimizer (the loss function for IEMOCAP is cross-entropy loss). The defaulted adjacency matrix is the indirect learning one. For the hyper-parameter setting, please refer to Table 3."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Baseline Evaluation: To ensure a fair comparison, for each baseline, following Gkoumas et al.[13], we reproduce the codes and determine the hyperparameters of the baseline by performing ifty-times random grid search on the hyperparameters, and the hyperparameter setting that reaches the best performance is saved. After the hyperparameters are determined, we train the model again with the best hyperparameters for ive times with diferent random seeds, and the inal results are obtained by calculating the mean results of the ive-time running. Our method follows the same procedure to obtain the inal results. Feature Extraction: For feature extraction, to make a fair comparison with baselines, we follow the setting of CMU-MultimodalSDK1. GloVe word embeddings [42] are used to extract the features of the transcripts in the videos. The Glove word embeddings, represent each word as a 300-dimensional vector, are trained on 840 billion tokens from the common crawl"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1https://github.com/A2Zadeh/CMU-MultimodalSDK"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 172, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "14"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Mai et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 94, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 3. Hyperparameters of Multimodal Graph."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "CMU-MOSI"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "CMU-MOSEI"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "IEMOCAP"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Batch Size "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Initial Learning Rate "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Training Epochs "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Gradient Clip "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Pooling Sequential Length (T\u2032) Feature Dimensionality (d) "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Convolution Layers (r)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "50 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.001 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "50 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.8 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "70 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "50 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "50 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.001 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "20 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.8 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "75 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "40 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "24 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.001 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "15 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.8 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "50 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "80 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 4. Comparison with RNN and TCN variants. The GRU and LSTM models used here are bidirectional. Training Time means training time of the model per batch (the batch size is the same for all models)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 170, "right": 144, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MAE"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Corr"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Training Time (s)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Parameters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "GRU "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "LSTM "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Regular TCN "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "1D-ResNet "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Dilated 1D-ResNet"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "80.4 80.7 62.9 79.8 80.4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 156, "right": 158, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "50.5 48.3 41.4 48.4 48.0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 156, "right": 138, "hanging": 18}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "80.5 81.3 77.2 80.1 80.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 132, "right": 132, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.611 0.623 0.838 0.626 0.636"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 138, "right": 138, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.669 0.662 0.007 0.646 0.644"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 138, "right": 138, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.933 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.774 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.120 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.134 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.127"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 576, "right": 576, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1,018,811 917,831 1,307,071 1,367,561 1,367,561"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "81.4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "49.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "81.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.608"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.675"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.125"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1,225,400"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "dataset. Facet2is used to extract a sequence of visual features that are composed of facial action units, facial landmarks, head pose, and gaze tracking, etc. COVAREP [8] is utilized for extracting acoustic features including 12 Mel-frequency cepstral coeicients, pitch tracking, speech polarity, glottal closure instants, spectral envelope, etc. These acoustic features are extracted from the full audio clip of each utterance to form a sequence that represents variations in the tone of voice across the utterance. We refer the reader to [49] for more details about the features."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 170, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.4 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Comparative Results "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "5.4.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Comparison with TCN and RNN. The mainstream approaches to processing sequences are RNN and TCN variants. Here we compare the proposed Multimodal Graph with RNN and TCN variants where the Unimodal Graphs and GPFN are replaced by them so as to investigate the efectiveness of GNNs on modeling sequence (for Transformer [51], please refer to Section ?? to see the comparison with Multimodal Transformer [49]). The TCN variants for comparison include the regular TCN that applies several 1-dimensional convolution layers to process the sequences[2], the ResNet counterpart that applies 1-dimensional convolution (1D-ResNet)[18], and the ResNet counterpart that applies 1-dimensional dilated convolution[61] (Dilated 1D-ResNet). The RNN variants for comparison include GRU [7] and LSTM [19]."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "We can infer from Table 4 that GRU [7] and LSTM [19] perform competitively, and Multimodal Graph still outperforms GRU and LSTM across the majority of the evaluation metrics. Speciically, Multimodal Graph reaches the best performance on binary accuracy, F1 score, MAE, and Corr metrics, and ranks second on the 7-class accuracy. For the TCN variants, the regular TCN without residual learning obtains the worst performance. 1D-ResNet and Dilated 1D-ResNet obtain satisfactory results and outperform the regular TCN by a signiicant margin, demonstrating the efectiveness of residual learning on building up a deep convolutional network to model sequences. Nevertheless, our Multimodal Graph still outperforms 1D-ResNet and Dilated 1D-ResNet,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 170, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2 iMotions 2017. https://imotions.com/"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "15"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 5. Performance of Multimodal Graph on CMU-MOSI and CMU-MOSEI datasets."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "CMU-MOSI"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "CMU-MOSEI"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Methods"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MAE"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Corr"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MAE"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Corr"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "LF-LSTM "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "EF-LSTM "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "TFN [64] "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "MFN [65] "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "MFRM [32] MulT [49] "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "MTAG [57]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "74.5 73.7 77.9 77.7 79.8 80.6 80.5"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 124, "right": 126, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "31.3 32.2 32.4 30.9 34.7 35.3 31.9"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 124, "right": 124, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "74.3 73.5 75.0 75.5 79.4 79.3 80.4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 120, "right": 118, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1.042 1.038 1.040 1.032 0.956 0.972 0.941"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 126, "right": 124, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.608 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.594 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.616 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.627 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.673 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.681 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.692"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "79.5 65.3 79.5 80.6 80.3 80.1 79.1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 142, "right": 142, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "48.0 41.7 49.3 49.1 48.9 49.0 48.2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 142, "right": 142, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "79.6 76.0 78.9 80.0 80.6 80.9 75.9"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 118, "right": 120, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.632 0.799 0.613 0.612 0.617 0.630 0.645"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 124, "right": 126, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.650 0.265 0.673 0.687 0.669 0.664 0.614"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "80.6"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "32.1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "80.5"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.933"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.684"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "81.4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "49.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "81.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.608"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.675"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 6. Comparison between Multimodal Graph and other approaches on IEMOCAP dataset."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Happy"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Sad"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Angry"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Neutral"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Average"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Models"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MFN [65] "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "TFN [67] "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "MulT [49] "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "MFRM [32] MTAG [57]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "79.7 77.1 85.7 85.9 85.8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 122, "right": 122, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "78.9 76.8 79.4 79.6 79.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 122, "right": 122, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "73.1 68.9 79.3 76.7 79.5"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 104, "right": 106, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "71.7 63.2 70.5 71.4 72.8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 106, "right": 104, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "72.8 71.7 75.8 76.0 76.2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 122, "right": 122, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "66.9 66.8 65.4 68.9 69.8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 120, "right": 122, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "57.9 51.0 51.8 55.1 57.8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 122, "right": 120, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "56.5 51.2 52.2 53.2 56.9"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 122, "right": 122, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "70.9 67.2 73.2 73.4 74.8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 120, "right": 122, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "68.5 64.5 66.9 68.3 69.8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 122, "right": 120, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "86.0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "81.3"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "79.4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "72.3"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "76.8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "71.9"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "61.4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "59.8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "75.9"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "71.3"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "yielding over 1% improvements on Acc7 and Acc2. These results demonstrate Multimodal Graph's superiority in modeling sequences. This is partly because Multimodal Graph can automatically learn long-term dependency with a suitable adjacency matrix that links distant related time steps. Moreover, diferent from the ixed modeling patterns of the TCNs and RNNs, for each time step, Multimodal Graph can identify and link various related time steps with it, which is more interpretable, lexible, and representative (see Appendix for detail). These results demonstrate GCN with an appropriate adjacency matrix as a novel and efective way of modeling sequences."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 168, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Analysis of Training Time and Parameters: Intuitively, since graph convolution dispenses with the recur-rence nature and allows parallel operation in the time dimension, it is faster than RNN networks. To verify this point, we report the training time per batch on the CMU-MOSEI dataset, as shown in Table 4. It can be seen that under the same experimental setting, training Multimodal Graph requires only 0.125s per batch, compared to 0.933s and 0.774s per batch for GRU and LSTM, respectively. Moreover, the training time of the three convolution networks (regular TCN, 1D-ResNet and Dilated 1D-ResNet) is close to that of our model. This empirically demon-strates that GCN is much more eicient than RNNs and is comparable to TCNs in terms of time complexity without sacriicing performance. We also report the number of trainable parameters to evaluate the space complexity of the models. It can be seen that GRU and LSTM require 1,018,811 and 917,831 parameters respectively, both fewer than GCNs. This is because we only implement three layers of GRU/LSTM which performs best according to our experiment. When we stack more layers, the performance of GRU/LSTM decreases, which is reasonable because RNNs generally are more diicult to train when model grows deeper. Additionally, the number of parameters of the three TCNs is slightly larger than that of our Multimodal Graph, demonstrating that the improvement of our model is not simply due to the increase in the number of parameters."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.4.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Comparison with Baselines for Multimodal Sentiment Analysis. We compare our Multimodal Graph with the competitive baselines on two benchmark datasets CMU-MOSI [67] and CMU-MOSEI [66],"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "16"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Mai et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 94, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "and the results are presented in Table 5. It can be seen that Multimodal Graph yields best results on most of the metrics on two datasets. Speciically, our model surpasses the state-of-the-art unaligned fusion method MulT[49] in terms of all metrics, except 7-class accuracy on CMU-MOSI. The results demonstrate the efectiveness of graph convolution and graph pooling in learning sequential data, compared to Transformer[51] which is employed in MulT. Moreover, our model also outperforms the graph-based method MTAG[57] across the majority of the evaluation metrics, further demonstrating the superiority of our method. We argue that this is partly because compared to MTAG that averages the nodes in the graph to perform graph readout (fusion), our Multimodal Graph applies graph pooling and devises novel pooling method to aggregate the multimodal nodes and learn high-level graph representation hierarchically."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Notably, the recent state-of-the-art aligned models [17, 35, 58] make great progress by using the large-pretrained BERT [9] to extract the language representation. In contrast, to make a fair comparison, we follow the state-of-the-art unaligned fusion models to use GloVe [42] to extract language embedding. Our method can also be extended to the aligned setting and uses BERT [9] to reach remarkable results. In our experiment, our model achieves a binary accuracy of 85.9% and a 7-way accuracy of 48.7% on CMU-MOSI dataset under the aligned setting, which suppresses the state-of-the-art models [17, 58]. Since our focus is unaligned fusion in this paper, we do not present the detailed results of aligned setting due to the lack of space."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Additionally, we discover that our Multimodal Graph performs better on larger dataset (CMU-MOSEI) than smaller one (CMU-MOSI). Similarly, the model performs better on IEMOCAP (see Section 5.4.3), which is a larger dataset, than on CMU-MOSI. These results show good generalizability of our model, which is scalable to large datasets."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 170, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.4.3 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Comparison with Baselines for Multimodal Emotion Recognition. We additionally evaluate the proposed method on the task of multimodal emotion recognition to justify the generalization ability of the model to other multimodal learning task. The widely-used dataset IEMOCAP is evaluated in this section. From the results presented in Table 6, it can be seen that the Multimodal Graph outperforms the baselines in the tasks of recognizing the 'Happy', 'Angry' and 'Neutral' emotions, yielding about 3.5% improvements compared with the best results of baselines on the recognizing of the 'Neutral' emotion. More importantly, Multimodal Graph outperforms the baselines in terms of the average performance, yielding over 1% improvements on the average accuracy and average F1 score. In addition to the task of multimodal sentiment analysis, the extra experiments of the more challenging multimodal emotion recognition task have proven the efectiveness and generalization ability of the proposed Multimodal Graph."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.4.4 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Ablation Studies. In this section, we perform ablation studies to investigate the efectiveness of the proposed graph convolution and graph pooling. From the results in Table 7, we discover that the introduced graph convolution and graph pooling are both beneicial to the performance of the model, without which the performance drops considerably. The graph convolution brings greater improvement than the graph pooling, indicating the importance of learning more expressive and discriminative unimodal representations. We also investigate the inluence of the proposed link similarity pooling by removing it from the GPFN (see the case of'W/O LSP in GPFN' in Table 7). The results suggest that the link similarity pooling is efective, which demonstrates the importance of a learnable graph pooling algorithm to automatically identify and cluster the related nodes in the multimodal graph hierarchically."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 158, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.4.5 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Analysis of Model Complexity. To analyze the model complexity of Multimodal Graph, we use the number of trainable parameters as the proxy for its space complexity, and compare it with the state-of-the-art unaligned fusion methods, as reported in Table 8. It can be seen that our Multimodal Graph requires 1,225,400 trainable parameters on CMU-MOSEI, which is 64.46% of the number of parameters of MulT. Although Multimodal Graph requires fewer trainable parameters than the current state-of-the-art model MulT, it still outperforms MulT"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "17"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 7. Ablation studies on CMU-MOSEI. In the case of 'W/O Graph Pooling', we remove the graph pooling layers in the GPFN. In the case of 'W/O Graph Convolution', we replace the graph convolution network with the fully connected layer to map the feature dimensionality of diferent modalities to be the same. 'W/O LSP in GPFN' denotes that the link similarity pooling is removed from the GPFN."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 170, "hanging": 4}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MAE"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Corr"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "W/O Graph Convolution W/O Graph Pooling "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "W/O LSP in GPFN"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "79.7 80.4 80.8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 164, "right": 164, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "48.2 49.6 49.2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 164, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "80.1 80.5 81.1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 140, "right": 138, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.639 0.612 0.610"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 146, "right": 144, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.645 0.671 0.674"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 146, "right": 144, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "81.4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "49.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "81.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.608"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.675"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 8. The Comparison of Model Complexity on CMU-MOSEI. We implement three widely-used baselines for com-parison in this section."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "hanging": 4}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Methods"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "TFN [64]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MulT [49]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MFN [65]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Number of Parameters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1,002,471"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1,901,161"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "963,777"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1,225,400"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 9. Discussion on the Concrete Graph Neural Networks on CMU-MOSEI. For the case of 'GAT' and 'GIN', we replace the defaulted graph convolution model GraphSAGE in Unimodal Graphs and GPFN with the corresponding GAT and GIN model. For the case of 'DifPool', we replace the max/mean graph pooling and link similarity pooling in GPFN with DifPool."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "hanging": 4}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Models"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MAE"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Corr"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "GAT [52] "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "GIN [56] "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "GraphSAGE [16]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "80.3 81.1 81.4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "49.0 49.0 49.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "80.4 81.6 81.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.627 0.615 0.608"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.650 0.676 0.675"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "DifPool [60] GPFN"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "81.1 81.4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "49.8 49.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "81.3 81.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.611 0.608"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.670 0.675"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "as shown in Section 5.4.2. This advantage in space complexity is signiicant because it shows that in addition to being efective in modeling sequential data, our Multimodal Graph is also more eicient than the variant of Transformer which is one dominant sequence learning method. This further validates GCN as a novel way of modeling sequential data. Compared to TFN and MFN, the proposed Multimodal Graph has more parameters. To sum up, given the high empirical performance of Multimodal Graph, the space complexity of Multimodal Graph is moderate compared to state-of-the-art unaligned sequence analysis methods."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.4.6 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Discussion of Diferent Graph Neural Networks. Since our Multimodal Graph is independent of the concrete GCN structure, we can choose any GCN structure to implement our model. This subsection compares diferent current GCN structures to analyze which kind of GCN structures performs best. Speciically, we compare the defaulted GraphSAGE [16] with Graph Attention Network (GAT) [52] and Graph Isomorphism Network (GIN) [56]. From Table 9 it can be seen that GraphSAGE [16] reaches the best performance among all the compared GCNs. In addition, the results of GIN [56] and GAT [52] are also satisfactory, indicating the generalization ability of Multimodal Graph."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "As for the graph pooling methods, we implement and evaluate DifPool [60] as the baseline to compare with the proposed GPFN. DifPool [60] achieves a relatively good result and is still inferior to our model. To be more"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "18"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Mai et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 94, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 10. Discussion on the adjacency matrices on CMU-MOSEI."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MAE"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Corr"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "All-one Matrix "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "KNN "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "GDM "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Direct Learning Indirect Learning"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "78.8 80.3 81.1 80.7 81.4"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "49.0 45.8 49.3 49.3 49.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "79.3 80.6 81.2 81.2 81.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.644 0.659 0.617 0.618 0.608"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.623 0.625 0.666 0.659 0.675"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "speciic, DifPool outperforms our GPFN in terms of 7-class accuracy by 0.1 points, while our model outperforms DifPool on the rest of the evaluation metrics. These results demonstrate the efectiveness of our graph pooling method."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.5 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Discussion of Adjacency Matrices "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "5.5.1 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "The Comparison of Diferent Adjacency Matrices. To analyze the performance of diferent kinds of adjacency matrices, we conduct an experiment where diferent types of adjacency matrix are used in Multimodal Graph. To show the efectiveness of the proposed types of adjacency matrix, i.e., indirect learning, direct learning, GDM and KNN-based adjacency matrices, we additionally implement one comparative trial where the adjacency matrix is an all-one matrix which corresponds to a fully-connected graph. We can infer from Table 10 that among all the adjacency matrices, the all-one matrix performs worst, which is understandable because the GCN in this case will reduce into a fully-connected graph with no ability to discern various relatedness between nodes. Besides, the KNN-based adjacency matrix reaches a relatively low performance, indicating that the Euclidean distance is not a perfect choice to determine the correlation between heterogeneous nodes. In contrast, both the indirect learning method and the GDM achieve satisfactory results. This is because GDM is in line with the general pattern of sequence modeling which sticks out the present time step and dilutes the past ones. And the direct learning method can learn such pattern in the absence of prior knowledge by gradient descent. Additionally, the indirect learning method, produces best results on all metrics. One possible reason is that it is both learnable and instance-speciic, and therefore it is more representative and can be optimized to capture more subtle and complex relatedness among nodes. To sum up, the comparative results that diferent types of adjacency matrices yield suggest that: (1) the proposed adjacency matrices are helpful in capturing useful information on the relatedness among nodes, compared to an all-one adjacency matrix that contains no such information, and (2) a learnable and meanwhile instance-speciic adjacency matrix is crucial for capturing more relatedness information among nodes."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5.5.2 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Visualization of Adjacency Matrices. To analyze the attributes of the indirect learning adjacency matrices, we provide a visualization of the unimodal and multimodal indirect learning adjacency matrices. Instead of merely analyzing a few instances, we average the adjacency matrices of all the testing instances in CMU-MOSEI to reveal the general patterns of the adjacency matrices, which is more convincing and informative."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Unimodal Adjacency Matrices: As can be inferred from Fig. 3, for the visual and acoustic modalities, the latter portion of nodes have much more impact than their counterparts in that they have more intensive link association (the nodes tends to have more connections with the latter portion of nodes), indicting that the model heavily relies on the latter portion of nodes for prediction and they are more informative. In contrast, the link association between diferent nodes in language modality is more even, indicting that the connections between diferent words are distributed evenly at nearly all temporal positions. Interestingly, the language adjacency"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 168, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "19"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(a) Indirect Learning (Language)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 708, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(b) Indirect Learning (Acoustic)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(c) Indirect Learning (Visual)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 414, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Fig. 3. Visualization of Unimodal Indirect Learning Adjacency Matrices. Each grid in the figure reflects the interaction between each corresponding two nodes. A darker color reflect a stronger interaction between the corresponding two nodes, and vice versa. For indirect learning, since the adjacency matrices are instance-specific, we average the adjacency matrices of the testing instances and visualize the mean adjacency matrix."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 154, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(a) Indirect Learning (Cross-modal) "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(b) KNN (Cross-modal)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1418, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Fig. 4. Visualization of Cross-modal Indirect Learning Adjacency Matrices. A darker color indicates a stronger inter-action, and vice versa. The labels of x and y axes are both 'nodes'. Since the KNN-based and indirect learning methods are both instance-specific, we average the adjacency matrices of the testing instances and visualize the mean adjacency matrices."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The first 50, the middle 500 and the later 500 nodes belong to language, acoustic, and visual modality, respectively."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 170, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "matrix suggests that the language nodes have fewer self-connections. This is reasonable because during the graph convolution, we add self-loop operation such that the nodes can connect with themselves (see Eq. 4), so the adjacency matrix does not need to learn a strong self-connection value on the diagonal, otherwise it would be redundant."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Cross-modal Adjacency Matrices: We also conduct visualization of the indirect learning adjacency matrix for multimodal sequence, and we provide the visualization of KNN-based adjacency matrix for comparison. As"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "20"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Mai et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 96, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "can be inferred from Fig. 4, for KNN-based method, the nodes from diferent modalities have no direct or close interactions with each other, and these nodes only interact with their neighboring nodes that come from the same modality. This means that by using the KNN-based adjacency matrix, the cross-modal interactions cannot be efectively explored, which may explain why it performs worse than other methods. The visualization of the KNN-based adjacency matrix also suggests that the distribution gap between diferent modalities actually exists and we need to handle it during modality fusion [31]. In contrast, for adjacency matrix of the indirect learning method, interestingly, the mean adjacency matrix for multimodal sequence suggests that the nodes tend to connect with the language nodes (i.e., the irst 50 nodes) more closely, which indicates that the language nodes are more important and informative. This scenario can be partly explained by the fact that language is more important than the other modalities, as revealed in [30, 43]. Additionally, as the adjacency matrix for the language nodes (the irst 50 nodes) has the darkest color, our visualization suggests the language nodes have the strongest connection with each other in the multimodal sequence."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 168, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "6 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "CONCLUSION"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "We employed popular GNNs to process multimodal sequences, which was free of the recurrent structure and proved more eicient and competitive. Speciically, we developed a unimodal graph for each modality to explore intra-modal dynamics, and a graph pooling fusion network over Unimodal Graphs to learn inter-modal dynamics. We proposed multiple ways to construct an adjacency matrix for a sequence. The experiments suggested that: 1) the proposed GCN-based model outperformed RNN and TCN variants with high computational eiciency, which indicated a new research direction in modeling sequences; 2) the proposed learnable and instance-speciic methods to deine an adjacency matrix performed best; 3) our method outperformed the transformer-based model MulT, which indicated the efectiveness of graph-based models; 4) the visualization results suggested that our model can identify important intra- and inter-modal interactions; 5) the proposed GPFN outperformed the classical graph pooling method DifPool in fusing cross-modal information. However, Multimodal Graph is non-causal, and thus not applicable to some language processing tasks. In the future, we aim to develop a causal GCN-based model to process sequences."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "hanging": 10}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "7 "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "ACKNOWLEDGMENT"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "This work was supported by the National Natural Science Foundation of China under Grant 62076262."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "REFERENCES"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[1] Shaojie Bai, J. Kolter, and Vladlen Koltun. 2019. Trellis Networks for Sequence Modeling. In Proceedings of International Conference on "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Learning Representations."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[2] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Sequence Modeling. Arxiv preprint Arxiv: 1803.01271 (2018)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[3] Tadas Baltru\u0161aitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2019. Multimodal Machine Learning: A Survey and Taxonomy. IEEE "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Transactions on Pattern Analysis and Machine Intelligence 41, 2 (Feb 2019), 423\u015b443. [4] Y Bengio, P Simard, and P Frasconi. 1994. Learning long-term dependencies with gradient descent is diicult. IEEE Transactions on "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Neural Networks 5, 2 (1994), 157\u015b166."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[5] Carlos Busso, Murtaza Bulut, Chi Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N. Chang, Sungbok Lee, and Shrikanth S. Narayanan. 2008. IEMOCAP: interactive emotional dyadic motion capture database. Language Resources and Evaluation 42, 4 (2008), 335\u015b359."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 516, "right": 156, "hanging": 268}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[6] Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Baltrus \u02c7aitis, Amir Zadeh, and Louis Philippe Morency. 2017. Multimodal sentiment analysis with word-level fusion and reinforcement learning. In 19th ACM International Conference on Multimodal Interaction (ICMI'17). 163\u015b171."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 516, "right": 144, "hanging": 268}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[7] Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 248, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. 1724\u015b1734."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 516, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "21"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[8] Gilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio, and Stefan Scherer. 2014. COVAREP: A Collaborative Voice Analysis "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Repository for Speech Technologies. In 2019 IEEE International Conference on Acoustics, Speech and Signal Processing. 960\u015b964. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Minnesota, 4171\u015b4186. "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[10] Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. 2019. Learning Discrete Structures for Graph Neural Networks. In "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "International conference on machine learning. "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[11] Sichao Fu, Weifeng Liu, Weili Guan, Yicong Zhou, Dapeng Tao, and Changsheng Xu. 2021. Dynamic Graph Learning Convolutional "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Networks for Semi-Supervised Classiication. ACM Trans. Multimedia Comput. Commun. Appl. 17, 1s, Article 4 (mar 2021), 13 pages."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[12] Difei Gao, Ke Li, R. Wang, S. Shan, and X. Chen. 2020. Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Text. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020), 12743\u015b12753."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[13] Dimitris Gkoumas, Qiuchi Li, C. Lioma, Yijun Yu, and Da wei Song. 2021. What makes the diference? An empirical comparison of "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "fusion strategies for multimodal language analysis. Information Fusion 66 (2021), 184\u015b197."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[14] M W Goudreau, C L Giles, S T Chakradhar, and . Chen, D. 1994. First-order versus second-order single-layer recurrent neural networks. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "IEEE Transactions on Neural Networks 5, 3 (1994), 511\u015b513."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[15] Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. 2006. Connectionist temporal classiication: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning. 369\u015b376."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 516, "right": 144, "hanging": 342}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[16] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances in neural information "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "processing systems. 1024\u015b1034."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[17] Devamanyu Hazarika, R. Zimmermann, and Soujanya Poria. 2020. MISA: Modality-Invariant and -Speciic Representations for Multimodal "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Sentiment Analysis. Proceedings of the 28th ACM International Conference on Multimedia (2020)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In IEEE Conference on "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Computer Vision and Pattern Recognition. 770\u015b778."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[19] Sepp Hochreiter and J1rgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997), 1735\u015b1780."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[20] Ming Hou, Jiajia Tang, Jianhai Zhang, Wanzeng Kong, and Qibin Zhao. 2019. Deep Multimodal Multilinear Fusion with High-order "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Polynomial Pooling. In Advances in Neural Information Processing Systems. 12113\u015b12122."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[21] Feiran Huang, Kaimin Wei, Jian Weng, and Zhoujun Li. 2020. Attention-Based Modality-Gated Networks for Image-Text Sentiment "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Analysis. ACM Trans. Multimedia Comput. Commun. Appl. 16, 3, Article 79 (jul 2020), 19 pages. [22] Rongrong Ji, Fuhai Chen, Liujuan Cao, and Yue Gao. 2019. Cross-Modality Microblog Sentiment Prediction via Bi-Layer Multimodal "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Hypergraph Learning. IEEE Transactions on Multimedia 21, 4 (2019), 1062\u015b1075. [23] Onno Kampman, Elham J. Barezi, Dario Bertero, and Pascale Fung. 2018. Investigating Audio, Visual, and Text Fusion Methods for "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "End-to-End Automatic Personality Prediction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(Volume 2: Short Papers)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[24] Diederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In Proceedings of International Conference on "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Learning Representations (ICLR)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[25] Thomas N Kipf and Max Welling. 2016. Semi-supervised classiication with graph convolutional networks. In Proceedings of International "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Conference on Learning Representations."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[26] Qiuchi Li, Dimitris Gkoumas, Christina Lioma, and Massimo Melucci. 2021. Quantum-inspired multimodal fusion for video sentiment "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "analysis. Information Fusion 65 (2021), 58\u015b71."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[27] Paul Pu Liang, Zhun Liu, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2019. Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 1569\u015b1576."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 516, "right": 174, "hanging": 342}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[28] Paul Pu Liang, Ziyin Liu, Amir Zadeh, and Louis Philippe Morency. 2018. Multimodal Language Analysis with Recurrent Multistage "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Fusion. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 150\u015b161."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[29] Zhun Liu, Ying Shen, Paul Pu Liang, Amir Zadeh, and Louis Philippe Morency. 2018. Eicient Low-rank Multimodal Fusion with "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Modality-Speciic Factors. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. 2247\u015b2256. [30] Sijie Mai, Haifeng Hu, and Songlong Xing. 2019. Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Global Perspectives for Multimodal Afective Computing. In Proceedings of the 57th Annual Meeting of the Association for Computational "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Linguistics. 481\u015b492."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[31] Sijie Mai, Haifeng Hu, and Songlong Xing. 2020. Modality to Modality Translation: An Adversarial Representation Learning and Graph "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Fusion Network for Multimodal Fusion. In Proceedings of the AAAI Conference on Artiicial Intelligence, Vol. 34. 164\u015b172."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "22"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Mai et al."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 96, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[32] S. Mai, H. Hu, J. Xu, and S. Xing. 2020. Multi-Fusion Residual Memory Network for Multimodal Human Sentiment Comprehension. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "IEEE Transactions on Afective Computing (2020), 1\u015b1."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[33] S. Mai, S. Xing, and H. Hu. 2020. Locally Conined Modality Fusion Network With a Global Perspective for Multimodal Human Afective "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Computing. IEEE Transactions on Multimedia 22, 1 (2020), 122\u015b137."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[34] Sijie Mai, Songlong Xing, and Haifeng Hu. 2021. Analyzing Multimodal Language via Acoustic-and Visual-LSTM with Channel-aware "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Temporal Convolution Network. IEEE/ACM Transactions on Audio, Speech, and Language Processing (2021)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[35] Sijie Mai, Ying Zeng, Shuangjia Zheng, and Haifeng Hu. 2022. Hybrid contrastive learning of tri-modal representation for multimodal "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "sentiment analysis. IEEE Transactions on Afective Computing (2022)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[36] Sijie Mai, Shuangjia Zheng, Yuedong Yang, and Haifeng Hu. 2021. Communicative message passing for inductive relation reasoning. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Association for the Advancement of Artiicial Intelligence (AAAI) (2021)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[37] A. Micheli. 2009. Neural Network for Graphs: A Contextual Constructive Approach. IEEE Transactions on Neural Networks 20, 3 (2009), "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "p.498\u015b511."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[38] Aashish Kumar Misraa, Ajinkya Kale, Pranav Aggarwal, and A. Aminian. 2020. Multi-Modal Retrieval using Graph Neural Networks. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "ArXiv abs/2010.01666 (2020)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[39] Behnaz Nojavanasghari, Deepak Gopinath, Jayanth Koushik, and Louis Philippe Morency. 2016. Deep multimodal fusion for persuasive-"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "ness prediction. In Proceedings of ACM International Conference on Multimodal Interaction. 284\u015b288."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[40] David Olson. 1977. From Utterance to Text: The Bias of Language in Speech and Writing. Harvard Educational Review 47, 3 (1977), "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "257\u015b281."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[41] Ashutosh Pandey and DeLiang Wang. 2019. TCNN: Temporal convolutional neural network for real-time speech enhancement in the "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "time domain. In 2019 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 6875\u015b6879."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[42] Jefrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "of the 2014 Conference on Empirical Methods in Natural Language Processing. 1532\u015b1543."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[43] Hai Pham, Paul Pu Liang, Thomas Manzini, Louis Philippe Morency, and Pocz\u02c7os Barnab\u02c7as. 2019. Found in Translation: Learning Robust "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Joint Representations by Cyclic Translations Between Modalities. In Thirty-Third AAAI Conference on Artiicial Intelligence. 6892\u015b6899. [44] Soujanya Poria, Erik Cambria, Devamanyu Hazarika, Navonil Majumder, Amir Zadeh, and Louis Philippe Morency. 2017. Context-"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Dependent Sentiment Analysis in User-Generated Videos. In Proceedings of the 55th Annual Meeting of the Association for Computational "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Linguistics. 873\u015b883."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[45] Soujanya Poria, Iti Chaturvedi, Erik Cambria, and Amir Hussain. 2016. Convolutional MKL Based Multimodal Emotion Recognition and "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Sentiment Analysis. In Proceedings of IEEE International Conference on Data Mining (ICDM). 439\u015b448."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[46] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. 2009. The Graph Neural Network Model. IEEE Transactions on "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Neural Networks 20, 1 (2009), 61\u015b80."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[47] Xiangbo Shu, Liyan Zhang, Yunlian Sun, and Jinhui Tang. 2020. Host\u015bparasite: graph LSTM-in-LSTM for group activity recognition. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "IEEE transactions on neural networks and learning systems 32, 2 (2020), 663\u015b674."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[48] Jinhui Tang, Xiangbo Shu, Zechao Li, Yu-Gang Jiang, and Qi Tian. 2019. Social anchor-unit graph regularized tensor completion for "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "large-scale image retagging. IEEE transactions on pattern analysis and machine intelligence 41, 8 (2019), 2027\u015b2034."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[49] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Multimodal Transformer for Unaligned Multimodal Language Sequences. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 6558\u015b6569."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 510, "right": 174, "hanging": 336}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[50] Yao-Hung Hubert Tsai, Martin Ma, Muqiao Yang, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2020. Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. 1823\u015b1833."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 516, "right": 156, "hanging": 342}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Attention is all you need. In Advances in neural information processing systems. 5998\u015b6008."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[52] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph attention networks. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "In Proceedings of International Conference on Learning Representations."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[53] Shiping Wang and Wenzhong Guo. 2017. Sparse Multigraph Embedding for Multimodal Feature Representation. IEEE Transactions on "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Multimedia 19, 7 (2017), 1454\u015b1466. "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[54] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. 2019. Words Can Shift: Dynamically "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Adjusting Word Representations Using Nonverbal Behaviors. In Proceedings of the AAAI Conference on Artiicial Intelligence, Vol. 33. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "7216\u015b7223."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[55] Martin Wollmer, Felix Weninger, Tobias Knaup, Bjorn Schuller, Congkai Sun, Kenji Sagae, and Louis Philippe Morency. 2013. YouTube "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Movie Reviews: Sentiment Analysis in an Audio-Visual Context. IEEE Intelligent Systems 28, 3 (2013), 46\u015b53."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[56] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful are graph neural networks?. In Proceedings of "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "International Conference on Learning Representations."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Multimodal Graph for Unaligned Multimodal Sequence Analysis Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2022"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "23"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[57] Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir Zadeh, Soujanya Poria, and Louis-Philippe Morency. 2020."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MTGAT: Multimodal Temporal Graph Attention Networks for Unaligned Human Multimodal Language Sequences. arXiv preprint arXiv:2010.11985 (2020)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 516, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[58] Kaicheng Yang, Hua Xu, and Kai Gao. 2020. CM-BERT: Cross-Modal BERT for Text-Audio Sentiment Analysis. In Proceedings of the 28th "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "ACM International Conference on Multimedia. 521\u015b528."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[59] Xiaocui Yang, Shi Feng, Daling Wang, and Yifei Zhang. 2020. Image-text Multimodal Emotion Classiication via Multi-view Attentional "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Network. IEEE Transactions on Multimedia (2020), 1\u015b1. "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[60] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. 2018. Hierarchical graph representation "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "learning with diferentiable pooling. In Advances in neural information processing systems. 4800\u015b4810."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[61] Fisher Yu and Vladlen Koltun. 2016. Multi-scale context aggregation by dilated convolutions. In Proceedings of International Conference "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "on Learning Representations."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[62] Hao Yuan and Shuiwang Ji. 2020. StructPool: Structured Graph Pooling via Conditional Random Fields. In Proceedings of International "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Conference on Learning Representations."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[63] Jiahong Yuan and Mark Liberman. 2008. Speaker identiication on the SCOTUS corpus. Acoustical Society of America Journal 123 (2008), "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "3878. "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "[64] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis Philippe Morency. 2017. Tensor Fusion Network for Multimodal "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Sentiment Analysis. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 1114\u015b1125."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[65] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis Philippe Morency. 2018. Memory Fusion "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Network for Multi-view Sequential Learning. In Proceedings of the AAAI Conference on Artiicial Intelligence (Vol. 32, No. 1). 5634\u015b5641. [66] Amir Zadeh, Paul Pu Liang, Jonathan Vanbriesen, Soujanya Poria, Edmund Tong, Erik Cambria, Minghai Chen, and Louis Philippe "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Morency. 2018. Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph. In Proceedings "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "of the 56th Annual Meeting of the Association for Computational Linguistics. 2236\u015b2246."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[67] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis Philippe Morency. 2016. Multimodal Sentiment Intensity Analysis in Videos: Facial "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Gestures and Verbal Messages. IEEE Intelligent Systems 31, 6 (11 2016), 82\u015b88."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[68] Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural networks. In Advances in Neural Information Processing "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Systems. 5165\u015b5175."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[69] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. An end-to-end deep learning architecture for graph classiication. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "In Thirty-Second AAAI Conference on Artiicial Intelligence."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 174, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "[70] Sicheng Zhao, Shangfei Wang, Mohammad Soleymani, Dhiraj Joshi, and Qiang Ji. 2019. Afective Computing for Large-Scale Het-erogeneous Multimedia Data: A Survey. ACM Trans. Multimedia Comput. Commun. Appl. 15, 3s, Article 93 (dec 2019), 32 pages."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 516, "right": 144, "hanging": 342}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ACM Trans. Multimedia Comput. Commun. Appl."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 174, "firstLine": 0}}}]}]}
{"TYPE": "document", "VALUE": [{"TYPE": "body", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "LegoNet: Efficient Convolutional Neural Networks with Lego Filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Zhaohui Yang1 2 *Yunhe Wang2Hanting Chen1 2 *Chuanjian Liu2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1840, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Boxin Shi3 4Chao Xu1Chunjing Xu2Chang Xu5"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 2692, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Abstract"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 2086, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "This paper aims to build efficient convolutional neural networks using a set of Lego filters. Many successful building blocks, e.g. inception and residual modules, have been designed to refresh state-of-the-art records of CNNs on visual recog-nition tasks. Beyond these high-level modules, we suggest that an ordinary filter in the neural net-work can be upgraded to a sophisticated module as well. Filter modules are established by assem-bling a shared set of Lego filters that are often of much lower dimensions. Weights in Lego filters and binary masks to stack Lego filters for these filter modules can be simultaneously optimized in an end-to-end manner as usual. Inspired by net-work engineering, we develop a split-transform-merge strategy for an efficient convolution by ex-ploiting intermediate Lego feature maps. The compression and acceleration achieved by Lego Networks using the proposed Lego filters have been theoretically discussed. Experimental results on benchmark datasets and deep models demon-strate the advantages of the proposed Lego filters and their potential real-world applications on mo-bile devices."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 402, "right": 432, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1. Introduction"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The success of deep convolutional neural networks (CNNs) has been well demonstrated on a wide variety of computer vision (CV) tasks, such as object recognition (Simonyan and Zisserman, 2014; Szegedy et al., 2015; He et al., 2016;"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 174, "hanging": 6}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "* This work was done when Zhaohui Yang and Hant-ing Chen were interns at Huawei Noah's Ark Lab. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "1Key Laboratory of Machine Perception (Ministry of Education), Peking University. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "2Huawei Noah's Ark Lab. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "3National"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Engineering"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Laboratory "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "for "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Video"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 180, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4Peng Cheng Laboratory."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Technology, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Peking"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 134, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5School of Com-"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 352, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "University"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "puter Science, University of Sydney."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Correspondence to:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Zhaohui Yang <zhaohuiyang@pku.edu.cn>,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Yunhe Wang"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "<wangyunhe@pku.edu.cn>, Chang Xu <c.xu@sydney.edu.au>."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Proceedings of the 36thInternational Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6, "right": 192, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Huang et al., 2017), detection (Ren et al., 2015; Liu et al., 2016), segmentation (He et al., 2017a), and tracking (Luo et al., 2017b). Due to the powerful feature expression ability of CNNs, building deeper networks will result in higher performance, but lead to the need for more resources. For instance, more than 90MB memory and 109FLOPs (floating-number operations) are required for launching the ResNet-50 (He et al., 2016), which limits the application of these deep neural networks on mobile phones, laptops, and other edge devices. Thus, we are motivated to explore portable deep neural networks with high performance."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A number of algorithms have been proposed to reduce mem-ory and FLOPs of convolutional networks with different concerns. (Han et al., 2015) introduced pruning, quanti-zation and Huffman coding for generating extremely com-pact deep models without obviously affecting their accu-racy. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(Jaderberg et al., 2014) used matrix factorization to decompose spatial structure of kernels. (Li et al., 2016) proposed to learn 2-bit weight and constructed binary neu-ral networks. (Molchanov et al., 2016) investigated Taylor expansions to eliminate side effects caused by removing filters. (He et al., 2017b) fine-tuned the pruned model for higher efficiency. (Gao et al., 2018) pruned useless chan-nels during the inference stage to accelerate. (Wang et al., 2018b) discarded redundant coefficients of parameters in the DCT frequency domain and introduced a data-driven method. (Xie et al., 2018) decompose convolution kernels along channel and spatial dimensions. (Bucilu\u02c7a et al., 2006; Hinton et al., 2015; Romero et al., 2014; Polino et al., 2018) introduced teacher-student distillation strategy. (Wu et al., 2018; Juefei-Xu et al., 2017) build light-weight network with depth-wise convolution. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(Wang et al., 2017) intro-duced circulant matrix to construct convolution kernels. Moreover, (Wu et al., 2016) compressed network based on product quantization, (Wang and Cheng, 2017) decomposed a weight matrix into two ternary matrices and a non-negative diagonal matrix to reduce memory and computational com-plexity. (Rastegari et al., 2016; Courbariaux et al., 2015) fur-ther studied the weight binarization problem in deep CNNs. Although these methods can produce very high compression and speed-up ratios, they often involve special architectures and operations (e.g. sparse convolution, fixed-point multipli-cation, and Huffman codebooks), which cannot be directly"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 162, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "LegoNet: Efficient Convolutional Neural Networks with Lego Filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "satisfied on off-the-shelf platforms and hardwares. Most importantly, these methods rely on a pre-trained network of heavy design and the performance of compressed models is usually upper bounded by this particular network."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 192, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Besides manipulating well-trained convolutional neural net-works, an alternative is to design efficient network architec-tures for learning representations. A set of network design principles have been developed in network engineering. For example, VGGNets (Simonyan and Zisserman, 2014) and ResNets (He et al., 2016) stack building blocks of the same shape, which reduces the free choices of hyper-parameters. Another important strategy is split-transform-merge in In-ception models (Szegedy et al., 2015), where the input is split into a few embeddings of lower dimensionalities, trans-formed by a set of specialized filters, and merged by concate-nation. The representational power of large and dense layers can therefore be approximated using this split-transform-merge strategy, while the computational complexity could be considerably lower. These insightful network design prin-ciples then produce a number of successful neural networks, e.g. Xception (Chollet, 2017), MobileNet (Howard et al., 2017). Shufflenet (Zhang et al., 2017), and ResNeXt (Xie et al., 2017). As filter is the basic unit in constructing deep neural networks, we must ask whether these network de-sign principles are applicable for re-designing filters in deep learning."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In this paper, we propose Lego Networks (LegoNets) that are efficient convolutional neural networks constructed with Lego filters. A set of lower-dimensional filters are discov-ered and taken as Lego bricks to be stacked for more com-plex filters, as shown in Fig. 1. Instead of manually stacking these Lego filters, we develop a method to learn the optimal permutation of Lego filters for a filter module. As these filter modules share the same set of Lego filter but with different combinations, we adapt the split-transform-merge strategy to accelerate their convolutions, which further de-crease the maximum serial FLOPS of standard convolution. Firstly, Lego filters are convolved with splitted part from input features, and then merge the convolved results. Ex-perimental results on benchmark datasets and CNN models demonstrate the superiority of the proposed Lego filters in establishing portable deep neural networks with acceptable performance."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2. Lego Network"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In this section, we first define the problem of how to com-press deep neural networks from a macro point of view. Then we introduce the concept of Lego Filters (LF), which are basic unit in our efficient convolutional networks. At last, we demonstrate the way of combining Lego filters."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(a) conv filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 346, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(b) Lego filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(c) stacked filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 1. The diagram of convolution filters represented by Lego filters. From left to right are conventional convolution filters, Lego filters with smaller sizes, and convolution filters stacked by exploiting a series of Lego filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "hanging": 10}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2.1. Lego Filters for Establishing CNNs"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Most of existing convolutional neural networks are over-parameterized with numerous parameters and enormous computational complexity. It is common to have more than one thousand of parameters in convolution filter (e.g. 3 \u00d73 \u00d7 128 = 1152), but it will produce only one convolution response for a given 3 \u00d7 3 input patch. There could be considerable redundancy in these learned convolution filters."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "To reduce the required number of parameters in deep neu-ral networks, some works proposed to decompose high-dimensional convolution filters into different efficient repre-sentations. For example, (Wu et al., 2016) exploited vector quantization. (Zhang et al., 2016) utilized singular value decomposition (SVD). (Kim et al., 2015) applied tensor de-composition, and (Cheng et al., 2015) replaced convolution filters by circulate matrices."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Although these schemes make tremendous efforts to repre-sent weights in deep CNNs with less parameters, most of them are proposed to compress and accelerate pre-trained neural networks, which cannot be directly applied for learn-ing CNNs from scratch. In addition, the performance of compressed neural networks is usually worse than that of original models, due to the loss caused by quantization or decomposition. Therefore, we are motivated to alleviate the redundancy between these filters during the stage of network design instead of waiting for solutions after all filters have been optimized through back propagation."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Given an arbitrary convolutional layer L with its n convo-lution filters F = {fi, ..., fn} \u2208 Rd\u00d7d\u00d7c\u00d7n, where d \u00d7 d is the size of filters and, c is the channel number, the convolu-tion operation can be formulated as"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 22, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Y = L(F, X), "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(1)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1920, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where X and Y are input data and output features of this layer. Convolution filters F are then solved from the follow-ing minimization problem by exploiting the feed-forward and back-propagation strategy, i.e."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 168, "right": 0, "hanging": 6}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u02c6F = arg min "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "F"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 598, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2|| \u02c6Y \u2212 L(F, X)||2 F"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 42, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(2)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 12, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "LegoNet: Efficient Convolutional Neural Networks with Lego Filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where\u02c6Y is the ground-truth of desired output of this layer, and || \u00b7 ||F is the Frobenius norm for matrices."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Here, we propose a set of smaller filters B = {b1, ..., bm} \u2208Rd\u00d7d\u00d7\u02dcc\u00d7mwith fewer channels (\u02dcc \u226a c), namely Lego filters, and apply them to establish"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F = G(b1, b2, ..., bm), "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(3)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1460, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where G is a linear transformation for stacking these Lego filters. Though F in Eq. 3 looks like a classical filter in Eq. 1, we take it as a filter module, as it is the assembled with Lego filters. Each Lego filter can be utilized for multiple times in constructing a filters module F, as shown in Figure 1. Hence, the attention of the optimization problem Eq. 2 has been turned to these Lego filters B of fewer parameters, i.e."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "1 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "\u02c6B = arg min "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "B "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "2||Y, L(G(B), X)||F 2. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(4)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 908, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "We can stack\u02c6B to construct F using Eq. 3 and then calculate the output data by exploiting the conventional convolution operation. The compression can be achieved if"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 194, "hanging": 8}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "d \u00d7 d \u00d7 \u02dcc \u00d7 m= c \u00d7 n\u02dcc \u00d7 m > 1."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 570, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(5)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 92, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Note that, there is a constraint over the number of channels of Lego filters. In practice, we need to select o = c/\u02dcc Lego filters from B to stack a general convolution filter, and o should be an integer for subsequent processing."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 194, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2.2. Learning to Stack Lego Filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A new convolution framework with Lego filters was pro-posed in Eq. 4 to reduce the space complexity of convolution filters in deep CNNs, and a transformation G for stacking Lego filters is proposed. In fact, we can design many ways to stack Lego filters, e.g. random projection and circulate matrix."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Admittedly, the optimal transformation G can be also learned during the training procedure of deep CNNs if it is exactly a linear projection, and different filters would have their own combinations of Lego filters. Dividing X into q = H\u2032\u00d7 W patches and verctorizing them, we have X = [vec(x1), ..., vec(xq)] \u2208 Rd2c\u00d7q. The output and fil-ters in L can be reformulated as Y = [vec(y1), ..., vec(yn)]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The conventional convolution operation as described in Eq. 1 and F = [vec(f1), ..., vec(fn)] \u2208 Rd2c\u00d7n, respectively."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "can be rewritten as"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Y = X\u22a4F. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(6)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1874, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Then, according to Eq. 3, we further divide the input data X into o = c/\u02dcc fragments [X1, ..., Xo], and stack m Lego filters to a matrix B = [vec(b1), ..., vec(bm)] \u2208 Rd2\u02dcc\u00d7m."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 158, "firstLine": 2}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Algorithm 1 Forward and Backward of LegoNet"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 160, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Require: Hyper-parameter o, m. Network architecture N. Total training iterations n. Learning rate \u03b7."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 88, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1: Initialize Lego Filters B, float gradient accumulator N"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 188, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "for each convolution layer L1, . . . , Lk by using o and"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 428, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2: for iter = 1 ...n do m. Task criterions C."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 188, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Get mini-batch data X, target Y."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Calculate M for each layer using N according to"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Eq. 9."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 626, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Construct convolution filters F for each layer using"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "lego filters B and binary matrix M. Filters are con-structed as F = BM(Eq. 3)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 626, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "6:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "7:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "8:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Forward LegoNet N(X) with stacked convo-lution kernels F, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "get prediction P, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Y "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "="}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd X\u22a4(BM)(Eq. 7). Calculate loss L using prediction P and ground truth"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Y, L = C(P, Y). Backward gradients related to parameters B and M,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 158, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "which denoted as \u2206B and \u2206M."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 620, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "9:"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "For each convolution layer, backward gradients M"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "to parameters N according to N using STE, which denoted as \u2206N."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 626, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "10: "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Update parameters B, N in Network N,\u02c6B = B \u2212"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 98, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "11: end for \u03b7 \u00d7 \u2206B,\u02c6N = N \u2212 \u03b7 \u00d7 \u2206N."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 98, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Ensure: Trained Network N\u2217"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 88, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Since output feature maps are calculated by accumulating convolution response extracted from all fragments of the in-put data, for the j-th feature maps Yjgenerated by the j-the convolution filter, i.e. the j-th column in F the convolution opearation using Lego filters can be formulated as"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Yj="}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 14, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "X\u22a4i(BMj i),"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 22, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(7)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 12, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "where Mj for selecting only one Lego filter from B to process the i\u2208 {0, 1}m\u00d71 and ||Mj i|| = 1 is a vector mask"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "i-th fragment of the input data. Therefore, the objective function for simultaneously learning Lego filters and their combination is"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 88, "right": 8, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "B,Mj min\ufffd2||Yj \u2212 X\u22a4i(BMj i)||2 F, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "1"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1080, "right": 576, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "s.t. Mj i\u2208 {0, 1}m\u00d71, ||Mj i||1 = 1, i = 1, ..., o."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 456, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(8)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 12, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "By minimizing the above function, we can obtain m Lego filters with corresponding n masks for stacking them to orig-inal convolution filters, as illustrated in Figure 1. By using masks, Lego filters could construct complete convolution filters as Fig. 1(c) shows."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "LegoNet: Efficient Convolutional Neural Networks with Lego Filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2.3. Optimization"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The proposed convolution operation needs the cooperation between Lego filters and stacking masks, which are to be optimized in the training procedure of deep neural network, i.e. B and M in Eq. 8. Since Mj matrix and optimizing M is a NP-hard problem, which i\u2208 {0, 1}m\u00d71 is a binary"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "makes it difficult to discover an optimal result using SGD. We thus proceed to relax the object function for learning M."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 144, "firstLine": 8}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "We introduce N \u2208 Rn\u00d7o\u00d7mwith the same shape as M. During training, M is binarized from N as follow,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Mj i,k=\ufffd1,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 940, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0, "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "if k = arg max Nj"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1784, "right": 1152, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "otherwise "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "i "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(9)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2140, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "s.t. j = 1, . . . , n, i = 1, . . . , o."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "During forward, Eq. 9 is used to produce binary mask M, however, it is a step function which is undifferentable. To enable gradients to pass through the binary mask, we refer to the Straight Through Estimator (STE) (Hubara et al., 2016). STE strategy is popular in training Binary Neural Network like (Rastegari et al., 2016). For any undifferentable trans-formation Eq. 9, the gradient \u2206N for float parameters N is same with the gradient \u2206M for output feature M."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Compared to Binary Neural Network with binarized weights and activations in {\u22121, 1}, our target matrix M's weights are in {0, 1}. Besides, there is no constraint on the number of each value in binary neural network, while we have the constraint M, ||Mj are concatenated brick by brick. The training pipeline is i||1 = 1, which constraints Lego filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 190, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "summarized in Alg. 1."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3. Efficient Implementation of Lego Filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A two-stage approach underlines Eq. 7, that is concatenation and convolution. Lego filters firstly construct convolution filters and apply convolution onto input feature maps. How-ever, repeated convolutions will be introduced during the convolution stage. For example, if two filter modules j1 and j2 contain same Lego filter at the same position, i.e. Mj1 convolution results will be exactly the same, i.e. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "i "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "= Mj2 i, i \u2264 o, as shown in Fig. 1(c) , their convolve"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 4, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "X\u22a4iM j1 = X\u22a4iM j2 i. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(10)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1514, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Towards an efficient convolution using Lego filters, we pro-pose a three stage pipeline, split-transform-merge. In the split stage, input feature maps are split into o fragments. In the transform stage, these fragments are convolved with each individual Lego filter, which leads to o \u00d7 m intermedi-ate feature maps in total. At last, these intermediate feature maps are summed according to M. We argue that this three stage convolution is equivalent to the aforementioned two-stage operation."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1. Split: We split input feature maps X \u2208 Rd2c\u00d7qinto o fragments [X1, . . . , Xo], where each fragment Xi \u2208Rd2\u02dcc\u00d7qwill be the basic feature map to be convolved with Lego filters."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 566, "right": 54, "hanging": 250}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2. Transform: Taking feature fragment Xi, i \u2264 o as the basic component for convolution, for each Lego filter "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Bj, j \u2264 m, we can calculate the convolution as"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 316, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Iij = X\u22a4iBj. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(11)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2154, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "By launching this convolution between each feature fragment and each Lego filter, there would be o \u00d7 m intermediate Lego feature maps Ii,j, i \u2264 o, j \u2264 m in total."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 566, "right": 54, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "We name this process as Lego Convolution, as the clas-sical convolution operation is split into convolutions over many smaller fragments cut from the original in-put feature map. Note that the major float operations are done in this stage, which could reduce the total number of float operations compared to the standard convolution operation."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 566, "right": 0, "hanging": 10}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3. Merge: In this stage, the desired output feature maps Y is produced from intermediate Lego feature maps I. However, in standard convolution, o different Lego kernels have to be concatenated for a complete convo-lution filter first, and then conduct convolutions with input feature mapX. In the above split and transform stages, we have pre-calculated convolution results be-tween input feature fragments and Lego filters and recorded them as intermediate Lego feature map. It is instructive to note that in previous two-stage con-volution, M is used to select Lego filter, while in the proposed three-stage pipeline, M is used for picking Lego feature maps from I and summarizing them."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 556, "right": 0, "hanging": 240}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Equivalent We next proceed to prove the equivalence be-tween the proposed efficient three-stage convolution and the standard convolutions using Lego filters in Theorem. 1."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Theorem 1. Suppose we reverse the convolution by split-transform-merge three-stage pipeline, the result should be equal to concat lego filters B using M to form standard kernels K and then convolve with feature map X."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Proof. In two-stage convolution, we calculate output feature maps X by firstly constructing a complete convolution filter and then conduct convolutions over input feature maps X,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 50, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Y = X\u22a4(BM) "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(12)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1862, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "For the j'th output Yj, the corresponding can be written as,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Yj="}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 14, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "X\u22a4i(BMj i)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 22, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(13)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 12, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "LegoNet: Efficient Convolutional Neural Networks with Lego Filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1444, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 2. Lego Unit(LU). This figure shows how the three-stage pipeline split-transform-merge operates on input feature maps. X is the input feature map, lego filters B are convolved with different parts from X, which result in intermediate feature maps I. Output feature map Y is generated by merging according to M."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 56, "hanging": 10}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "From the perspective of matrix, Yjcan be calculated as,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Yj="}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 18, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\ufffd(X\u22a4iB)Mj i."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 38, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(14)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 112, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "The first item X\u22a4iB denotes the intermediate Lego feature"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "map I. M selects feature maps from I and summarizes them"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "to generate output Y. Hence, the proposed split-transform-"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "merge strategy can result in the same output feature map."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A convolution layer reformed by the proposed split-transform-merge pipeline can be equivalent to the two-stage construct-convolve solution. By using this split-transform-merge pipeline, repeated computations are eliminated, and the network could feed forward efficiently. Efficient Lego networks can be established using Lego Unit shown in Fig. 2."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4. Analysis on Compression Performance"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Compared with standard convolution, convolution kernels constructed by Lego filters can greatly reduce the number of parameters. Further, by using our proposed split-transform-merge convolution strategy for Lego filters, neural network calculation could be accelerated. In this section, we analyze the memory and float operations in detail."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.1. Compression"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "We define the size of the convolution kernel F as d2\u00d7 c \u00d7 n and the size of the input feature map X as d2 that input channel into o segments and have m Lego Filters x\u00d7c. We divide"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 2, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "at hand. All parameters are saved with float-32 data type except for M matrix with binary weights. The compression rate is calculated by"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 198, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "m \u00d7c o\u00d7 d2 + n \u00d7 o \u00d7 m+ \u2248 n \u00d7 o . "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(15)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 886, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In the denominator, m \u00d7c o\u00d7 d2 denotes the memory occu-pied by Lego filter takes. n \u00d7 o \u00d7 m is the memory for M. Since the binary matrix M is relatively small compared to"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "the Lego filter parameters, the total compression ratio for"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "convolution layer would be approximatelyn\u00d7o m."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 88, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "By using Lego filters to construct filter modules according to binary matrix M, we can save a large volume of parame-ters, which make compressed network applicable for mobile devices."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 162, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.2. Acceleration"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 162, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In order to accelerate inference time, we develop the split-transform-merge strategy, which can largely reduce the num-ber of float operations in LegoNet. For a standard convolu-tion layer, float operations number is calculated as"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 162, "right": 22, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "n \u00d7 c \u00d7 d2\u00d7 d2 x, "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "(16)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1808, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "For Lego networks, firstly generating standard convolution filter using Lego filters B and binary matrix M, and then conducting convolve as usual would not increase any extra float operations. Hence, Eq. 16 provides an upper bound of the number of float operations. In other words, even in the worst case, applying Lego filters will not increase computational burden."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 162, "right": 56, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In some cases, if the number of Lego filters m is smaller than the output channel number n, it could be optimized using split-transform-merge strategy. The theoretical speed up for an optimized convolution layer can be calculated as"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 162, "right": 52, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "LegoNet: Efficient Convolutional Neural Networks with Lego Filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "n \u00d7 c \u00d7 d2\u00d7 d2 x"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 832, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "m \u00d7 o \u00d7c o\u00d7 d2 \u00d7 d2x+ n \u00d7 o \u00d7 d2x"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 450, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "\u2248n m."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 50, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(17)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 92, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "m \u00d7 o \u00d7c o\u00d7 d2 k\u00d7 d2 xis number of float operations required"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "by Lego convolution. It would generate m \u00d7 o intermediate Lego features with channel equal to 1. In the merge stage,"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "n \u00d7 o \u00d7 d2 maps. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "xFLOPS are taken to sum up these Lego feature"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "5. Experiments"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 14, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Experiment Setup. We have developed a novel convo-lution framework using Lego filters in the above section, here we will first test the proposed method on the CIFAR-10 (Krizhevsky and Hinton, 2010) dataset with different parameters and settings. The CIFAR-10 dataset consists of 60000 natural images with 32 \u00d7 32 resolution split into train/test fold. We select the VGGNet-16 (Simonyan and Zisserman, 2014) as the baseline model, which is a classi-cal deep neural network and has a 93.25% accuracy on the CIFAR-10 benchmark. This network contains 13 convolu-tion layers, followed by 3 fully-connected layers, which is widely used in visual recognition, detection and segmenta-tion tasks. In order to apply VGGNet-16 on the CIFAR10 dataset, we replace the last convolutional layer by a global average pooling layer and then reconfigure a fully-connected layer for conducting the classification task with 10 cate-gories. The network will be trained 1,000 epochs with the batch size of 128 using the conventional SGD. The initial learning rate is set as 0.1, which will be reduced by a fac-tor of 10 at 200, 400, 600, 800, 900 epochs, respectively. Weight decay is set to 5 \u00d7 10\u22124for regularization. The momentum parameter is set to 0.9. In addition, images in the training set are firstly padded by 4 pixels, and 32 \u00d7 32 patches will be randomly sampled for padded images for data augmentation. Code is available at github1."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Binary v.s. Weighted. Conventional filters in CNNs are divided into two parts by using the proposed approach, i.e. Lego filters B and corresponding binary mask M for record-ing their permutations. In order to solve these two vari-ables efficiently, an intermediate variable N was introduced in Eq. 9 for relaxing the constrain of the binary mask M. Therefore, besides to permute Lego filters to conventional filters using M, we also can utilize N to obtain another permutation of Lego filters with o \u00d7 n floating numbers to assign each Lego filter a learnable weight. Considering that, there are d2\u00d7 \u02dcc \u00d7 m parameters in the given conventional layer, these coefficients do not account for an obvious pro-portion for storing the entire neural network. Thus, we first test the performance of Lego networks with and without additional coefficients on Lego filters."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1https://github.com/zhaohui-yang/LegoNet pytorch"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 268, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 3. Impact of two parameters o and m, o indicates how many fragments input feature maps are splitted into. m is set to 0.125, 0.25 and 0.5 times to the original output features. Upper line is LegoNet with coefficients, which verify the impact of introduced coefficients."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 170, "right": 0, "hanging": 10}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Impact of Parameters. We evaluate the performance of our proposed LegoNet as described in the previous section on CIFAR10 dataset."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 170, "right": 56, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Lego filters could construct convolution filters with and without coefficients while concatenating, in order to full explore the impact of coefficients, we test LegoNet with a range of compression ratios. We set hyper-parameter o to be 2 for whole network, which indicates that input fea-tures are splitted into two fragments, m \u2208 R+indicates the ratio of Lego filters compared to the original output chan-nels. We set m ranging from 0.125 to 0.5, which compress VGGNet-16 by a factor of 4-16\u00d7. Fig. 3 shows the results, for any compression ratio, Lego filters concatenating with coefficients(denoted as o = 2, coeff) always performs bet-ter than directly concatenating without coefficients(denoted as o = 2, w/o coeff). Under same parameters budget, by introducing few more coefficients, LegoNet would enhance the expression ability by a large margin. As compression ratio increases, coefficients play an more important role, in the extreme compression situation of 16\u00d7, LegoNet with coefficient could maintain performance about 90% accuracy, compared to 86% accuracy of LegoNet without coefficient. We argue that if parameters are not too few, parameters are enough to learn comparable results, e.g. , Lego-VGGNet-16-w(o=2,m=0.5) in Tab. 1. However, if VGGNet-16 is compressed extremely, using Lego filters would introduce many repeat calculations among different filter modules. We thus need few coefficients for weighted concatenation to strength the expression ability. Further experiments are all conducted with coefficients during concatenating."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 156, "right": 0, "firstLine": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "There are two parameters o and m in LegoNet, i.e. , o indi-cates how many fragments input feature maps are splitted into, m indicates the number of Lego filters compared to the"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "LegoNet: Efficient Convolutional Neural Networks with Lego Filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 1. Comparison results of different neural networks on the CIFAR-10 datasets."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Model"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acc (%)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Params(M)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Comp ratio"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "FLOPS(M)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Speed Up"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "VGGNet-16(Simonyan and Zisserman, 2014)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "93.25"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "14.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "298.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Lego-VGGNet-16-w(o=2,m=0.5) Lego-VGGNet-16-w(o=2,m=0.25) Lego-VGGNet-16-w(o=4,m=0.5) Lego-VGGNet-16-w(o=4,m=0.25)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 432, "right": 432, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "93.23 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "91.97 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "92.42 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "91.35"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 228, "right": 214, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "3.7 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "1.9 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "1.9 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.9"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 406, "right": 406, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "8\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "8\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 404, "right": 404, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "149.4 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "74.7 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "149.4 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "74.7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "2\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "4\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 350, "right": 354, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "16\u00d7"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "4\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6124, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "original of each layer. We conduct our experiments on dif-ferent o and m which compress VGGNet-16 by a factor of 4-64\u00d7. Fig. 3 shows the relationship between performance and two parameters."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "As mentioned above, Lego-VGGNet-16-w could compress the network by a factor of m/o. When we set different o or m to achieve a compression ratio less than 8\u00d7, accuracy drops less than 1%, e.g. , Lego-VGGNet-16-w(o=4, m=0.5) in Tab. 1. As the parameter grows, the accuracy will in-crease, which in consistent with our motivation, however, this will lead to larger model size or much more flops. There is thus a trade-off between accuracy, model size and speed."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In order to analysis the relationship between params and flops, as previous figures show, under the same budget of parameters, the performance are approximately the same. The number of parameters directly indicates the final perfor-mance of the network. Under the budget of approximately same parameters, higher o which indicates much more frag-ments, which could achieve higher performance, e.g. , Lego-VGGNet-16-w(o=2, m = 0.25) achieves 91.97% accuracy, which is almost the same accuracy with Lego-VGGNet-16-w(o=4, m = 0.5) with 92.42% accuracy. However, float operations vary a lot for two networks. Lego-VGGNet-16-w(o=4, m = 0.5) costs twice flops compared to model Lego-VGGNet-16-w(o=2, m = 0.25). Note that using our proposed three-stage strategy by split-transform-merge, the number of FLOPS is proportional to the number of Lego filters for each layer. Thus, under same parameters budget, though larger o introduce higher performance, but takes much more flops. Take flops into consideration, in order to balance the model size and flops, we set o = 2 in the rest of our experiments, which reduce a large amount of flops while maintain comparable accuracy."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Large-Scale Datasets. We test our LegoNet on a large-scale classification task, ILSVRC2012, with several dif-ferent architectures. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "We evaluate LegoNet based on ResNet50 (He et al., 2016), VGGNet-16 (Simonyan and Zisserman, 2014) and MobileNet (Howard et al., 2017) net-work architecture. Images are resized with 256 pixels for shorter side and 224 \u00d7 224 pixels patchs are randomly sampled from resized image as data augmentation. Each batch contains 256 images for training. Center crop is used for testing. We trained 300 epochs in total. Learning rate started with 10\u22121and decayed by a factor of 10 every 80"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "epochs. Note that although the last fully connected layer for classification has lots of parameters, if we use Lego fil-ters to compress the last layer, many classes would share similar features, which would introduce side effect on per-formance, especially fine-grained classification. Besides, if the network backbone is used in tasks like detection and segmentation, the last fully connected layer is replaced by other layers, so we do not compress the last fully connected layer in all our experiments."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In VGGNet-16 network, 138M parameters are mainly occu-pied by fully connected layer, which requires a large amount of memory resources. However, this could be removed by introducing Global Average Pooling(GAP) after all con-volution layers, about 10% parameters left after removing fully connected layers, with almost same accuracy. Then a 1000 classes fully connected layer followed by a soft-max layer is used for classification. We apply our Lego filters onto VGGNet-16-GAP network. Lego-VGGNet-16-w(o=2,m=0.5) compressed original VGGNet-16 by a fac-tor of approximately 30\u00d7 and achieved comparable per-formance as Tab. 2 shows. Such a small model would be sufficient for mobile devices. As VGGNet-16 network has been largely used in many different computer vision tasks, deploying such a small Lego-VGGNet-16-w(o=2,m=0.5) could satisfy most of the needs. 2\u00d7 float operations are reduced which speed up inference time."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ResNet50 usually contains 1x1 and 3x3 convolutions. 1x1 convolution layers are mainly used for channel-wise trans-formation and 3x3 convolution is used to merge spatial features. We used the same compression method as that on CIFAR10, thus setting parameter o to be 2 and controls the number of Lego filters m. We compress two types of layers without difference. In the ResNet50 network, the convolutional feature extractor is followed by classification layer of the network. The final classification layer occupies 2M parameters. Tab. 2 shows Lego-Res50 with 2-3 \u00d7 com-pression ratio. Accuracy keeps to be almost the same with 3\u00d7 compression ratio. Meanwhile, float operations reduced a lot in these networks by approximately 2\u00d7. Compared to the original, Lego-Res50-w(o=2,m=0.5) is a more portable alternative to the original."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In addition, we evaluate LegoNet-Res50 with and without coefficients on large scale dataset. Compared to weighted concatenation of Lego filters, Lego-Res50(o=2,m=0.5)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 40, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "LegoNet: Efficient Convolutional Neural Networks with Lego Filters "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Table 2. Comparison results of different neural networks on the ILSVRC2012 datasets."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 1728, "right": 1728, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Model"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Top-5 Acc(%)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Params(M)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Comp Ratio"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "FLOPs(B)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Speed Up"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ResNet50 (He et al., 2016) "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "ThiNet-Res (Luo et al., 2017a) Versatile (Wang et al., 2018a) Lego-Res50(o=2,m=0.5) "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Lego-Res50-w(o=2,m=0.5) "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Lego-Res50-w(o=2,m=0.6)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 576, "right": 576, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "92.2 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "88.3 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "91.8 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "89.7 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "90.6 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "91.3"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 486, "right": 472, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "25.6 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "8.7 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "11.0 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "8.1 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "8.1 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "9.3"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1.0\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2.9\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2.3\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "3.2\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "3.2\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2.8\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 366, "right": 368, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.1 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2.2 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "3.0 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2.0 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2.0 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2.0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 378, "right": 376, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1.0\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "1.9\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "1.4\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2.0\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2.0\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "1.7\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 284, "right": 288, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "VGGNet-16 (Simonyan and Zisserman, 2014) ThiNet-VGG (Luo et al., 2017a) "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Lego-VGGNet-16-w(o=2,m=0.5) "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Lego-VGGNet-16-w(o=2,m=0.6)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "90.1 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "90.3 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "88.9 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "89.2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 486, "right": 472, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "138.0 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "38.0 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "4.2 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "5.0"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1.0\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "3.6\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "32.9\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "27.6\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "15.3 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "3.9 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "7.7 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "9.2"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1.0\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "3.9\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2.0\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "1.7\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 284, "right": 288, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "MobileNet (Howard et al., 2017) Lego-Mobile-w(o=2,m=0.9) "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Lego-Mobile-w(o=2,m=1.5)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 576, "right": 576, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "88.9 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "87.5 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "88.3"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 486, "right": 472, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "4.2 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "2.5 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "3.5"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 406, "right": 406, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1.0\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "1.7\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 288, "right": 288, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "0.6 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.5 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "0.6"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 378, "right": 376, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1.0\u00d7"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "1.1\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 144, "right": 288, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "1.2\u00d7"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "1.0\u00d7"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 6470, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 480, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "CT_Empty", "VALUE": "[w:drawing]"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 28, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "data, which achieves same float operations and inference"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 548, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "time as the original."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 548, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Generalization Ability."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 548, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In order to full explore the gen-"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 102, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "eralization ability of LegoNet, we evaluate our LegoNet"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 548, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "on VOC object detection task(Everingham et al., 2010)."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 548, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Faster-RCNN (Ren et al., 2015) was used as the detection"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 548, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "framework, VOC07 train+val dataset was used to train the"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 548, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "network. We used Lego-Res50-w(o=2,m=0.5) as the detec-"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 548, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "tion backbone. Tab. 3 shows the results of trained network."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 548, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(a) FRCNN"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 408, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "(b) Lego-FRCNN"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 198, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Comparing baseline network, our LegoNet achieves compa-"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 548, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": []}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "rable results with much less parameters."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 548, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Figure 4. Example object detection results on PASCAL VOC dataset."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 14, "right": 144, "hanging": 10}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "drops 2% more accuracy, which proves that the introduced coefficients indeed improve LegoNet expression ability. For VGGNet-16 and MobileNet, we only tested the performance which contains coefficients."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 8, "right": 190, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Further, we adopt proposed LegoNet onto mobile setting networks, MobileNet (Howard et al., 2017). VGGNet-16 and ResNet50 are designed for a higher classification perfor-mance. Given much more parameters, higher accuracy can be achieved. Compressing these kinds of networks while preserving their performance is easier than compressing MobileNet like efficient network designs."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 14, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Mobilenet consists depthwise convolutional filters and point-wise convolution which are 3\u00d73 and 1\u00d71 convolution. Depthwise convolution learns a transform for each chan-nel using 3\u00d7 3 convolution with group number equals to input channels. 1\u00d71 pointwise convolution takes most of the parameters in MobileNet, thus we mainly adopt our Lego filters onto those 1\u00d71 convolution. We test our proposed Lego-MobileNet-w on ILSVRC2012 and achieved less than 1% accuracy drop with 1.2\u00d7 compression. Note that for model Lego-Mobile-w(o=2,m=1.5), the number of Lego filters for each layer is 1.5\u00d7 compared to the original, di-rectly using our proposed split-transform-merge three-stage pipeline, flops is larger than the original. For this model, it reaches the upper bound and we forward this network by firstly reconstruct convolution filters and then forward input"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Table 3. Object detection results on the VOC2007 dataset."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 418, "right": 0, "firstLine": 0}}}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Model"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "mAP(%)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Params(M)"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "table", "VALUE": [{"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "ResNet50"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "72.8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "23.8"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}, {"TYPE": "table-row", "VALUE": [{"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Lego-Res50(o=2,m=0.5)-w"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "71.3"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}, {"TYPE": "table-cell", "VALUE": [{"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "6.3"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}]}]}]}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Here, we give the example of Faster-RCNN detection result. It can be seen from Fig. 4 that the difference between the original ResNet50 and our LegoNet is quite small."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "6. Conclusion"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "In this work, we propose a new method to construct ef-ficient convolutional neural networks with a set of Lego filters. We first define the problem of network compression from the perspective of how to construct convolution filters with a shared set of Lego filters. Then we propose a learn-ing method to simultaneously optimize binary masks and weights in end-to-end training stage. We further develop a split-transform-merge three-stage strategy for efficient con-volution. We evaluate LegoNet with different backbones and compare their performance, parameters, float operations and speed up. The proposed LegoNet could combine with any state-of-the-art architecture and can be easily deployed onto mobile devices."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 160, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Acknowledgement"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "This work is supported by National Natural Science Foun-dation of China under Grant No. 61876007, 61872012 and Australian Research Council Project DE-180101438."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 166, "right": 0, "hanging": 6}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "LegoNet: Efficient Convolutional Neural Networks with Lego Filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "References"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "C. Bucilu\u02c7a, R. Caruana, and A. Niculescu-Mizil. Model compres-sion. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535-541. ACM, 2006."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 144, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary, and S.-F. Chang. An exploration of parameter redundancy in deep net-works with circulant projections. In Proceedings of the IEEE In-ternational Conference on Computer Vision, pages 2857-2865, 2015."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 164, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F. Chollet. Xception: Deep learning with depthwise separable "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "convolutions. arXiv preprint, pages 1610-02357, 2017."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect: Train-ing deep neural networks with binary weights during propaga-tions. In Advances in neural information processing systems, pages 3123-3131, 2015."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 166, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-serman. The pascal visual object classes (voc) challenge. Inter-national journal of computer vision, 88(2):303-338, 2010."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 166, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "X. Gao, Y. Zhao, L. Dudziak, R. D. Mullins, and C. Xu. Dynamic channel pruning: Feature boosting and suppression. arXiv: Computer Vision and Pattern Recognition, 2018."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 144, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 196, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 164, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "K. He, G. Gkioxari, P. Doll\u00b4ar, and R. Girshick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE International Conference on, pages 2980-2988. IEEE, 2017a."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 204, "right": 196, "hanging": 194}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Y. He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. international conference on computer vision, pages 1398-1406, 2017b."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 188, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "neural network. arXiv preprint arXiv:1503.02531, 2015."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 144, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 204, "right": 144, "hanging": 194}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In CVPR, volume 1, page 3, 2017."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 164, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized neural networks. In Advances in neural information processing systems, pages 4107-4115, 2016."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 144, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up con-volutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 206, "right": 144, "hanging": 196}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "F. Juefei-Xu, V. Naresh Boddeti, and M. Savvides. Local binary convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 19-28, 2017."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 188, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin. Com-pression of deep convolutional neural networks for fast and low power mobile applications. arXiv preprint arXiv:1511.06530, 2015."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 364, "right": 0, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A. Krizhevsky and G. Hinton. Convolutional deep belief networks "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "on cifar-10. Unpublished manuscript, 40(7), 2010."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 164, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 364, "right": 0, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21-37. Springer, 2016."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 358, "right": 50, "hanging": 194}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "J.-H. Luo, J. Wu, and W. Lin. Thinet: A filter level pruning method for deep neural network compression. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 5068-5076. IEEE, 2017a."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 364, "right": 50, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "W. Luo, P. Sun, F. Zhong, W. Liu, and Y. Wang. End-to-end active object tracking via reinforcement learning. arXiv preprint arXiv:1705.10561, 2017b."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 364, "right": 50, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz. Pruning convolutional neural networks for resource efficient transfer learning. CoRR, abs/1611.06440, 2016."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 364, "right": 46, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A. Polino, R. Pascanu, and D. Alistarh. Model compression via distillation and quantization. arXiv preprint arXiv:1802.05668, 2018."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 364, "right": 0, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, pages 525-542. Springer, 2016."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 364, "right": 0, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91-99, 2015."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 364, "right": 0, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 354, "right": 50, "hanging": 190}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "K. Simonyan and A. Zisserman. Very deep convolutional net-"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "works for large-scale image recognition. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "arXiv preprint "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "arXiv:1409.1556, 2014."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 164, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1-9, 2015."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 364, "right": 0, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "P. Wang and J. Cheng. Fixed-point factorized networks. computer "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "vision and pattern recognition, pages 3966-3974, 2017."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 164, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Y. Wang, C. Xu, C. Xu, and D. Tao. Beyond filters: Compact feature map for portable deep model. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3703-3711. JMLR. org, 2017."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 364, "right": 50, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Y. Wang, C. Xu, X. Chunjing, C. Xu, and D. Tao. Learning versatile filters for efficient convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1615-1625, 2018a."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 360, "right": 50, "hanging": 196}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "LegoNet: Efficient Convolutional Neural Networks with Lego Filters"}], "style": {"indent": {"TYPE": "CT_Ind", "left": 0, "right": 0, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "Y. Wang, C. Xu, C. Xu, and D. Tao. Packing convolutional neural "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "networks in the frequency domain. IEEE transactions on pattern "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "analysis and machine intelligence, 2018b."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 5070, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "B. Wu, A. Wan, X. Yue, P. Jin, S. Zhao, N. Golmant, A. Gho-"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "laminejad, J. Gonzalez, and K. Keutzer. Shift: A zero flop, "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "zero parameter alternative to spatial convolutions. In Proceed-"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "ings of the IEEE Conference on Computer Vision and Pattern "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Recognition, pages 9127-9135, 2018."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 5040, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "J. Wu, C. Leng, Y. Wang, Q. Hu, and J. Cheng. Quantized convolu-"}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "tional neural networks for mobile devices. In Proceedings of the "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "IEEE Conference on Computer Vision and Pattern Recognition, "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "pages 4820-4828, 2016."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 5040, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "G. Xie, T. Zhang, K. Yang, J. Lai, and J. Wang. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Decoupled "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "convolutions for cnns. In Thirty-Second AAAI Conference on "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Artificial Intelligence, 2018."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 5040, "firstLine": 0}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "S. Xie, R. Girshick, P. Doll\u00b4ar, Z. Tu, and K. He. Aggregated "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "residual transformations for deep neural networks. In Computer "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "Vision and Pattern Recognition (CVPR), 2017 IEEE Conference "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "on, pages 5987-5995. IEEE, 2017."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 5070, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very deep "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "convolutional networks for classification and detection. IEEE "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "transactions on pattern analysis and machine intelligence, 38 "}, {"TYPE": "Break"}, {"TYPE": "text", "VALUE": "(10):1943-1955, 2016."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 210, "right": 5070, "hanging": 200}}}, {"TYPE": "paragraph", "VALUE": [{"TYPE": "text", "VALUE": "X. Zhang, X. Zhou, M. Lin, and J. Sun. "}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "Shufflenet: An ex-"}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "tremely efficient convolutional neural network for mobile de-"}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "vices. CoRR, abs/1707.01083, 2017. URL . "}, {"TYPE": "Break"}, {"TYPE": "TabChar"}, {"TYPE": "text", "VALUE": "."}], "style": {"indent": {"TYPE": "CT_Ind", "left": 10, "right": 4896, "firstLine": 0}}}]}]}